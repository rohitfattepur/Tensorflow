{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SO_Home.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SKWwG5iU_Bt2",
        "iutilLUp9AsZ",
        "sjwfJo493IAV",
        "DEyyro-xMnwS",
        "5f2C617e-BFu",
        "g_jgkP3sdF4r",
        "_Dqzlnvmwrr3",
        "q_FhnDE9wzlF",
        "eVLkvAkPatDD",
        "pb4cJKwt2eIK",
        "z_5aAAf82LIE",
        "Iah4GD0dbRBW",
        "-1vXRhpz0mou",
        "Mk6am3SF58J6",
        "2Ip33pHpNOVU",
        "SVB8mbWfNCIF",
        "4s_VHK_PNWCO",
        "Wjw2f-GyThTk",
        "hzPFRXDg4Di0",
        "0fARWvSOS0xo",
        "sUvdRR4EyO6-",
        "nj1v2ptOIvyc",
        "nvI2gcXuUZ_A",
        "nyAx1Zp1Us4F",
        "4ecy2vH3X6gU",
        "H0zJAoNLU7pu",
        "5u1qthgoVMUL",
        "7x-CnHNIH0r1",
        "xDqxUBulbZua",
        "3yg9AIIvdD7b",
        "OwhjPAkYvkBV",
        "aWjlUrHl0Nr1",
        "n9_kieW013Bh",
        "XjDDrht_4wca",
        "jr6jOpefsMfJ"
      ],
      "authorship_tag": "ABX9TyMf+nTryem+SppTc1J2USdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitfattepur/Tensorflow/blob/master/SO_Home.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKWwG5iU_Bt2",
        "colab_type": "text"
      },
      "source": [
        "# Unwanted Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWPhM3KGN4TA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "Loss = 0\n",
        "\n",
        "def loss1(y_true1,y_pred1):\n",
        "    return np.square(np.subtract(y_true1,y_pred1)).mean()\n",
        "\n",
        "def loss2(y_true2,y_pred2):\n",
        "    return np.square(np.subtract(y_true2,y_pred2)).mean()\n",
        "\n",
        "def finalloss(y_pred1, y_true1, y_pred2, y_true2):\n",
        "    Loss = loss1(y_pred1, y_true1) + loss2(y_pred2, y_true2)\n",
        "    if(y_pred1 == y_true1 and y_pred2 == y_true2):\n",
        "       return(0)\n",
        "    elif(y_pred1 == y_true1 and y_pred2 != y_true2):\n",
        "      return(0.5 * Loss)\n",
        "    elif(y_pred1 != y_true1 and y_pred2 == y_true2):\n",
        "      return(0.5 * Loss)    \n",
        "    else:\n",
        "      return(Loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmquaW0AoiYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a1573d5-9109-4d33-f766-9fd2f83059ca"
      },
      "source": [
        "finalloss(1,1,7,2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxQKG938OI4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZcVrTbBOOA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n1sw4uUOUq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCQEsTdrOaj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "#for epoch in range(19, 21):\n",
        "train(model, device, train_loader, optimizer, epoch = 20)\n",
        "test(model, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24tVvrWxp2kU",
        "colab_type": "code",
        "outputId": "c4f7d1fa-4e6b-46ca-b471-7255708f62ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "print(\"Tensorflow Version:\",tf.__version__)\n",
        "  \n",
        "model = MobileNetV2(input_shape=[128, 128, 3], include_top=False) #or whatever model\n",
        "\n",
        "print(\"Weights of the Layer\",model.trainable_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow Version: 1.15.0\n",
            "Weights of the Layer [<tf.Variable 'Conv1_1/kernel:0' shape=(3, 3, 3, 32) dtype=float32>, <tf.Variable 'bn_Conv1_1/gamma:0' shape=(32,) dtype=float32>, <tf.Variable 'bn_Conv1_1/beta:0' shape=(32,) dtype=float32>, <tf.Variable 'expanded_conv_depthwise_1/depthwise_kernel:0' shape=(3, 3, 32, 1) dtype=float32>, <tf.Variable 'expanded_conv_depthwise_BN_1/gamma:0' shape=(32,) dtype=float32>, <tf.Variable 'expanded_conv_depthwise_BN_1/beta:0' shape=(32,) dtype=float32>, <tf.Variable 'expanded_conv_project_1/kernel:0' shape=(1, 1, 32, 16) dtype=float32>, <tf.Variable 'expanded_conv_project_BN_1/gamma:0' shape=(16,) dtype=float32>, <tf.Variable 'expanded_conv_project_BN_1/beta:0' shape=(16,) dtype=float32>, <tf.Variable 'block_1_expand_1/kernel:0' shape=(1, 1, 16, 96) dtype=float32>, <tf.Variable 'block_1_expand_BN_1/gamma:0' shape=(96,) dtype=float32>, <tf.Variable 'block_1_expand_BN_1/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'block_1_depthwise_1/depthwise_kernel:0' shape=(3, 3, 96, 1) dtype=float32>, <tf.Variable 'block_1_depthwise_BN_1/gamma:0' shape=(96,) dtype=float32>, <tf.Variable 'block_1_depthwise_BN_1/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'block_1_project_1/kernel:0' shape=(1, 1, 96, 24) dtype=float32>, <tf.Variable 'block_1_project_BN_1/gamma:0' shape=(24,) dtype=float32>, <tf.Variable 'block_1_project_BN_1/beta:0' shape=(24,) dtype=float32>, <tf.Variable 'block_2_expand_1/kernel:0' shape=(1, 1, 24, 144) dtype=float32>, <tf.Variable 'block_2_expand_BN_1/gamma:0' shape=(144,) dtype=float32>, <tf.Variable 'block_2_expand_BN_1/beta:0' shape=(144,) dtype=float32>, <tf.Variable 'block_2_depthwise_1/depthwise_kernel:0' shape=(3, 3, 144, 1) dtype=float32>, <tf.Variable 'block_2_depthwise_BN_1/gamma:0' shape=(144,) dtype=float32>, <tf.Variable 'block_2_depthwise_BN_1/beta:0' shape=(144,) dtype=float32>, <tf.Variable 'block_2_project_1/kernel:0' shape=(1, 1, 144, 24) dtype=float32>, <tf.Variable 'block_2_project_BN_1/gamma:0' shape=(24,) dtype=float32>, <tf.Variable 'block_2_project_BN_1/beta:0' shape=(24,) dtype=float32>, <tf.Variable 'block_3_expand_1/kernel:0' shape=(1, 1, 24, 144) dtype=float32>, <tf.Variable 'block_3_expand_BN_1/gamma:0' shape=(144,) dtype=float32>, <tf.Variable 'block_3_expand_BN_1/beta:0' shape=(144,) dtype=float32>, <tf.Variable 'block_3_depthwise_1/depthwise_kernel:0' shape=(3, 3, 144, 1) dtype=float32>, <tf.Variable 'block_3_depthwise_BN_1/gamma:0' shape=(144,) dtype=float32>, <tf.Variable 'block_3_depthwise_BN_1/beta:0' shape=(144,) dtype=float32>, <tf.Variable 'block_3_project_1/kernel:0' shape=(1, 1, 144, 32) dtype=float32>, <tf.Variable 'block_3_project_BN_1/gamma:0' shape=(32,) dtype=float32>, <tf.Variable 'block_3_project_BN_1/beta:0' shape=(32,) dtype=float32>, <tf.Variable 'block_4_expand_1/kernel:0' shape=(1, 1, 32, 192) dtype=float32>, <tf.Variable 'block_4_expand_BN_1/gamma:0' shape=(192,) dtype=float32>, <tf.Variable 'block_4_expand_BN_1/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'block_4_depthwise_1/depthwise_kernel:0' shape=(3, 3, 192, 1) dtype=float32>, <tf.Variable 'block_4_depthwise_BN_1/gamma:0' shape=(192,) dtype=float32>, <tf.Variable 'block_4_depthwise_BN_1/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'block_4_project_1/kernel:0' shape=(1, 1, 192, 32) dtype=float32>, <tf.Variable 'block_4_project_BN_1/gamma:0' shape=(32,) dtype=float32>, <tf.Variable 'block_4_project_BN_1/beta:0' shape=(32,) dtype=float32>, <tf.Variable 'block_5_expand_1/kernel:0' shape=(1, 1, 32, 192) dtype=float32>, <tf.Variable 'block_5_expand_BN_1/gamma:0' shape=(192,) dtype=float32>, <tf.Variable 'block_5_expand_BN_1/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'block_5_depthwise_1/depthwise_kernel:0' shape=(3, 3, 192, 1) dtype=float32>, <tf.Variable 'block_5_depthwise_BN_1/gamma:0' shape=(192,) dtype=float32>, <tf.Variable 'block_5_depthwise_BN_1/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'block_5_project_1/kernel:0' shape=(1, 1, 192, 32) dtype=float32>, <tf.Variable 'block_5_project_BN_1/gamma:0' shape=(32,) dtype=float32>, <tf.Variable 'block_5_project_BN_1/beta:0' shape=(32,) dtype=float32>, <tf.Variable 'block_6_expand_1/kernel:0' shape=(1, 1, 32, 192) dtype=float32>, <tf.Variable 'block_6_expand_BN_1/gamma:0' shape=(192,) dtype=float32>, <tf.Variable 'block_6_expand_BN_1/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'block_6_depthwise_1/depthwise_kernel:0' shape=(3, 3, 192, 1) dtype=float32>, <tf.Variable 'block_6_depthwise_BN_1/gamma:0' shape=(192,) dtype=float32>, <tf.Variable 'block_6_depthwise_BN_1/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'block_6_project_1/kernel:0' shape=(1, 1, 192, 64) dtype=float32>, <tf.Variable 'block_6_project_BN_1/gamma:0' shape=(64,) dtype=float32>, <tf.Variable 'block_6_project_BN_1/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'block_7_expand_1/kernel:0' shape=(1, 1, 64, 384) dtype=float32>, <tf.Variable 'block_7_expand_BN_1/gamma:0' shape=(384,) dtype=float32>, <tf.Variable 'block_7_expand_BN_1/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'block_7_depthwise_1/depthwise_kernel:0' shape=(3, 3, 384, 1) dtype=float32>, <tf.Variable 'block_7_depthwise_BN_1/gamma:0' shape=(384,) dtype=float32>, <tf.Variable 'block_7_depthwise_BN_1/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'block_7_project_1/kernel:0' shape=(1, 1, 384, 64) dtype=float32>, <tf.Variable 'block_7_project_BN_1/gamma:0' shape=(64,) dtype=float32>, <tf.Variable 'block_7_project_BN_1/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'block_8_expand_1/kernel:0' shape=(1, 1, 64, 384) dtype=float32>, <tf.Variable 'block_8_expand_BN_1/gamma:0' shape=(384,) dtype=float32>, <tf.Variable 'block_8_expand_BN_1/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'block_8_depthwise_1/depthwise_kernel:0' shape=(3, 3, 384, 1) dtype=float32>, <tf.Variable 'block_8_depthwise_BN_1/gamma:0' shape=(384,) dtype=float32>, <tf.Variable 'block_8_depthwise_BN_1/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'block_8_project_1/kernel:0' shape=(1, 1, 384, 64) dtype=float32>, <tf.Variable 'block_8_project_BN_1/gamma:0' shape=(64,) dtype=float32>, <tf.Variable 'block_8_project_BN_1/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'block_9_expand_1/kernel:0' shape=(1, 1, 64, 384) dtype=float32>, <tf.Variable 'block_9_expand_BN_1/gamma:0' shape=(384,) dtype=float32>, <tf.Variable 'block_9_expand_BN_1/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'block_9_depthwise_1/depthwise_kernel:0' shape=(3, 3, 384, 1) dtype=float32>, <tf.Variable 'block_9_depthwise_BN_1/gamma:0' shape=(384,) dtype=float32>, <tf.Variable 'block_9_depthwise_BN_1/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'block_9_project_1/kernel:0' shape=(1, 1, 384, 64) dtype=float32>, <tf.Variable 'block_9_project_BN_1/gamma:0' shape=(64,) dtype=float32>, <tf.Variable 'block_9_project_BN_1/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'block_10_expand_1/kernel:0' shape=(1, 1, 64, 384) dtype=float32>, <tf.Variable 'block_10_expand_BN_1/gamma:0' shape=(384,) dtype=float32>, <tf.Variable 'block_10_expand_BN_1/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'block_10_depthwise_1/depthwise_kernel:0' shape=(3, 3, 384, 1) dtype=float32>, <tf.Variable 'block_10_depthwise_BN_1/gamma:0' shape=(384,) dtype=float32>, <tf.Variable 'block_10_depthwise_BN_1/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'block_10_project_1/kernel:0' shape=(1, 1, 384, 96) dtype=float32>, <tf.Variable 'block_10_project_BN_1/gamma:0' shape=(96,) dtype=float32>, <tf.Variable 'block_10_project_BN_1/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'block_11_expand_1/kernel:0' shape=(1, 1, 96, 576) dtype=float32>, <tf.Variable 'block_11_expand_BN_1/gamma:0' shape=(576,) dtype=float32>, <tf.Variable 'block_11_expand_BN_1/beta:0' shape=(576,) dtype=float32>, <tf.Variable 'block_11_depthwise_1/depthwise_kernel:0' shape=(3, 3, 576, 1) dtype=float32>, <tf.Variable 'block_11_depthwise_BN_1/gamma:0' shape=(576,) dtype=float32>, <tf.Variable 'block_11_depthwise_BN_1/beta:0' shape=(576,) dtype=float32>, <tf.Variable 'block_11_project_1/kernel:0' shape=(1, 1, 576, 96) dtype=float32>, <tf.Variable 'block_11_project_BN_1/gamma:0' shape=(96,) dtype=float32>, <tf.Variable 'block_11_project_BN_1/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'block_12_expand_1/kernel:0' shape=(1, 1, 96, 576) dtype=float32>, <tf.Variable 'block_12_expand_BN_1/gamma:0' shape=(576,) dtype=float32>, <tf.Variable 'block_12_expand_BN_1/beta:0' shape=(576,) dtype=float32>, <tf.Variable 'block_12_depthwise_1/depthwise_kernel:0' shape=(3, 3, 576, 1) dtype=float32>, <tf.Variable 'block_12_depthwise_BN_1/gamma:0' shape=(576,) dtype=float32>, <tf.Variable 'block_12_depthwise_BN_1/beta:0' shape=(576,) dtype=float32>, <tf.Variable 'block_12_project_1/kernel:0' shape=(1, 1, 576, 96) dtype=float32>, <tf.Variable 'block_12_project_BN_1/gamma:0' shape=(96,) dtype=float32>, <tf.Variable 'block_12_project_BN_1/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'block_13_expand_1/kernel:0' shape=(1, 1, 96, 576) dtype=float32>, <tf.Variable 'block_13_expand_BN_1/gamma:0' shape=(576,) dtype=float32>, <tf.Variable 'block_13_expand_BN_1/beta:0' shape=(576,) dtype=float32>, <tf.Variable 'block_13_depthwise_1/depthwise_kernel:0' shape=(3, 3, 576, 1) dtype=float32>, <tf.Variable 'block_13_depthwise_BN_1/gamma:0' shape=(576,) dtype=float32>, <tf.Variable 'block_13_depthwise_BN_1/beta:0' shape=(576,) dtype=float32>, <tf.Variable 'block_13_project_1/kernel:0' shape=(1, 1, 576, 160) dtype=float32>, <tf.Variable 'block_13_project_BN_1/gamma:0' shape=(160,) dtype=float32>, <tf.Variable 'block_13_project_BN_1/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'block_14_expand_1/kernel:0' shape=(1, 1, 160, 960) dtype=float32>, <tf.Variable 'block_14_expand_BN_1/gamma:0' shape=(960,) dtype=float32>, <tf.Variable 'block_14_expand_BN_1/beta:0' shape=(960,) dtype=float32>, <tf.Variable 'block_14_depthwise_1/depthwise_kernel:0' shape=(3, 3, 960, 1) dtype=float32>, <tf.Variable 'block_14_depthwise_BN_1/gamma:0' shape=(960,) dtype=float32>, <tf.Variable 'block_14_depthwise_BN_1/beta:0' shape=(960,) dtype=float32>, <tf.Variable 'block_14_project_1/kernel:0' shape=(1, 1, 960, 160) dtype=float32>, <tf.Variable 'block_14_project_BN_1/gamma:0' shape=(160,) dtype=float32>, <tf.Variable 'block_14_project_BN_1/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'block_15_expand_1/kernel:0' shape=(1, 1, 160, 960) dtype=float32>, <tf.Variable 'block_15_expand_BN_1/gamma:0' shape=(960,) dtype=float32>, <tf.Variable 'block_15_expand_BN_1/beta:0' shape=(960,) dtype=float32>, <tf.Variable 'block_15_depthwise_1/depthwise_kernel:0' shape=(3, 3, 960, 1) dtype=float32>, <tf.Variable 'block_15_depthwise_BN_1/gamma:0' shape=(960,) dtype=float32>, <tf.Variable 'block_15_depthwise_BN_1/beta:0' shape=(960,) dtype=float32>, <tf.Variable 'block_15_project_1/kernel:0' shape=(1, 1, 960, 160) dtype=float32>, <tf.Variable 'block_15_project_BN_1/gamma:0' shape=(160,) dtype=float32>, <tf.Variable 'block_15_project_BN_1/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'block_16_expand_1/kernel:0' shape=(1, 1, 160, 960) dtype=float32>, <tf.Variable 'block_16_expand_BN_1/gamma:0' shape=(960,) dtype=float32>, <tf.Variable 'block_16_expand_BN_1/beta:0' shape=(960,) dtype=float32>, <tf.Variable 'block_16_depthwise_1/depthwise_kernel:0' shape=(3, 3, 960, 1) dtype=float32>, <tf.Variable 'block_16_depthwise_BN_1/gamma:0' shape=(960,) dtype=float32>, <tf.Variable 'block_16_depthwise_BN_1/beta:0' shape=(960,) dtype=float32>, <tf.Variable 'block_16_project_1/kernel:0' shape=(1, 1, 960, 320) dtype=float32>, <tf.Variable 'block_16_project_BN_1/gamma:0' shape=(320,) dtype=float32>, <tf.Variable 'block_16_project_BN_1/beta:0' shape=(320,) dtype=float32>, <tf.Variable 'Conv_1_1/kernel:0' shape=(1, 1, 320, 1280) dtype=float32>, <tf.Variable 'Conv_1_bn_1/gamma:0' shape=(1280,) dtype=float32>, <tf.Variable 'Conv_1_bn_1/beta:0' shape=(1280,) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0asFNBlrQG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
        "        self.des = tf.constant([[1.,2.]])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        # y = self.des\n",
        "        return self.dense2(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl7e6GCXrkCf",
        "colab_type": "code",
        "outputId": "de7f73c1-c86c-4cbc-b24b-578b5094cc26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = MyModel()\n",
        "print(model.trainable_variables)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW7Ue9vDxQyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "inputs = np.ones((10, 5)) \n",
        "outs = model(inputs) \n",
        "print(model.trainable_variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdn72dnmLCJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (1) Importing dependency\n",
        "    import keras\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "    from keras.layers.normalization import BatchNormalization\n",
        "    import numpy as np\n",
        "    np.random.seed(1000)\n",
        "    \n",
        "    # (2) Get Data\n",
        "    import tflearn.datasets.oxflower17 as oxflower17\n",
        "    x, y = oxflower17.load_data(one_hot=True)\n",
        "    \n",
        "    # (3) Create a sequential model\n",
        "    model = Sequential()\n",
        "    \n",
        "    # 1st Convolutional Layer\n",
        "    model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "    # Batch Normalisation before passing it to the next layer\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 2nd Convolutional Layer\n",
        "    model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 3rd Convolutional Layer\n",
        "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 4th Convolutional Layer\n",
        "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 5th Convolutional Layer\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # Passing it to a dense layer\n",
        "    model.add(Flatten())\n",
        "    # 1st Dense Layer\n",
        "    model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "    model.add(Activation('relu'))\n",
        "    # Add Dropout to prevent overfitting\n",
        "    model.add(Dropout(0.4))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 2nd Dense Layer\n",
        "    model.add(Dense(4096))\n",
        "    model.add(Activation('relu'))\n",
        "    # Add Dropout\n",
        "    model.add(Dropout(0.4))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 3rd Dense Layer\n",
        "    model.add(Dense(1000))\n",
        "    model.add(Activation('relu'))\n",
        "    # Add Dropout\n",
        "    model.add(Dropout(0.4))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # Output Layer\n",
        "    model.add(Dense(17))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    #model.summary()\n",
        "    \n",
        "    # (4) Compile \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    # (5) Define Gradient Function\n",
        "    def get_gradient_func(model):\n",
        "        grads = K.gradients(model.total_loss, model.trainable_weights)\n",
        "        inputs = model.model._feed_inputs + model.model._feed_targets + model.model._feed_sample_weights\n",
        "        func = K.function(inputs, grads)\n",
        "        return func\n",
        "    \n",
        "    # (6) Train the model such that gradients are captured for every epoch\n",
        "    epoch_gradient = []\n",
        "    for epoch in range(1,5):\n",
        "        model.fit(x, y, batch_size=64, epochs= epoch, initial_epoch = (epoch-1), verbose=1, validation_split=0.2, shuffle=True)\n",
        "        inputs = model.model._feed_inputs + model.model._feed_targets + model.model._feed_sample_weights\n",
        "        print(model.input)\n",
        "        print(model.total_loss)\n",
        "        get_gradient = get_gradient_func(model)\n",
        "        grads = get_gradient([x, y, np.ones(len(y))])\n",
        "        epoch_gradient.append(grads)\n",
        "    \n",
        "    # (7) Convert to a 2 dimensiaonal array of (epoch, gradients) type\n",
        "    gradient = np.asarray(epoch_gradient)\n",
        "    #print(\"Total number of epochs run:\", epoch)\n",
        "    #print(\"Gradient Array has the shape:\",gradient.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBnu4AJrnW_c",
        "colab_type": "code",
        "outputId": "a69e2966-fc1b-4ac1-8f55-3f2209128c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#!pip install tensorflow==1.14\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "print(\"tensorflow version:\",tf.__version__)\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(7,(3,3) , padding = \"same\" , input_shape = (28,28,1)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Dense(50,activation = 'relu'))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    return model \n",
        "\n",
        "model_discriminator = make_discriminator_model()\n",
        "\n",
        "print(\"I'm a Symbolic tensor:\",model_discriminator)\n",
        "\n",
        "#initialize the variable\n",
        "init_op = tf.initialize_all_variables()\n",
        "\n",
        "#run the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init_op) #execute init_op\n",
        "    print(\"Value of the model_discriminator function:\",sess.run(model_discriminator(np.random.rand(1,28,28,1).astype(\"float32\"))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow version: 1.14.0\n",
            "I'm a Symbolic tensor: <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f5a4d62d438>\n",
            "Value of the model_discriminator function: [[0.00674586]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHuGn1xmoRUB",
        "colab_type": "code",
        "outputId": "dd898966-fb8e-455d-8ce6-535a412cf406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "v = tf.Variable(1)\n",
        "\n",
        "#@tf.function ... \n",
        "def f(x): \n",
        "  ta = tf.TensorArray(tf.int32, size=0, dynamic_size=True)\n",
        "  for i in tf.range(x): \n",
        "      v.assign_add(i) \n",
        "      ta = ta.write(i, v) \n",
        "  return ta.stack()\n",
        "\n",
        "f(5)\n",
        "ta = tf.TensorArray(tf.int32, size=10, dynamic_size=True)\n",
        "print(ta)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f379a0c1dd8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9AfVGylOzPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib \n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWuIrI1tQ3Gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "graph=K.get_session().graph\n",
        "\n",
        "graph_def=graph.as_graph_def()\n",
        "print(graph_def)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmAGpwpFaVh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import Image\n",
        "import ImageChops\n",
        "\n",
        "im1 = Image.open(\"splash.png\")\n",
        "im2 = Image.open(\"splash2.png\")\n",
        "\n",
        "diff = ImageChops.difference(im2, im1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS-rGAUnZzZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow==2.1\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "graph=K.get_session().graph\n",
        "\n",
        "graph_def=graph.as_graph_def()\n",
        "print(graph_def)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCfwsUo8j7j7",
        "colab_type": "code",
        "outputId": "1252ad90-73ce-4171-efbe-90593e1c68e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#!pip install tensorflow==2.1\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def traceme(x):\n",
        "    return model(x)\n",
        "\n",
        "\n",
        "logdir = '/tmp/tensorboard1/'\n",
        "writer = tf.summary.create_file_writer(logdir)\n",
        "tf.summary.trace_on(graph=True, profiler=True)\n",
        "\n",
        "# Forward pass\n",
        "traceme(tf.zeros((1, 28, 28, 1)))\n",
        "with writer.as_default():\n",
        "    tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir=logdir)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Trace already enabled\n",
            "WARNING:tensorflow:Model was constructed with shape Tensor(\"flatten_9_input:0\", shape=(None, 28, 28), dtype=float32) for input (None, 28, 28), but it was re-called on a Tensor with incompatible shape (1, 28, 28, 1).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O889UWBikfYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir==logdir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o1gfEKP41Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.ops import summary_ops_v2\n",
        "from tensorflow.python.keras.backend import get_graph\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "tb_path = '/tmp/tensorboard/'\n",
        "tb_writer = tf.summary.create_file_writer(tb_path)\n",
        "with tb_writer.as_default():\n",
        "    if not model.run_eagerly:\n",
        "        summary_ops_v2.graph(get_graph(), step=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxsivbfXDhEW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_cH7lAzlOiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensorboard --logdir=tb_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqMrpp1JDjRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Comes from Generative Deep Learning by David Foster\n",
        "class RandomWeightedAverage(tf.keras.layers.Layer):\n",
        "    def __init__(self, batch_size):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
        "    def call(self, inputs):\n",
        "        alpha = K.random_uniform((self.batch_size, 1, 1, 1))\n",
        "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
        "\n",
        "# Dummy critic\n",
        "def critic():\n",
        "    critic = Sequential()\n",
        "    inputShape = (28, 28, 1)\n",
        "\n",
        "    critic.add(Conv2D(32, (5, 5), padding=\"same\", strides=(2, 2),\n",
        "        input_shape=inputShape))\n",
        "    critic.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    critic.add(Conv2D(64, (5, 5), padding=\"same\", strides=(2, 2)))\n",
        "    critic.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    critic.add(Flatten())\n",
        "    critic.add(Dense(512))\n",
        "    critic.add(LeakyReLU(alpha=0.2))\n",
        "    critic.add(Dropout(0.3))\n",
        "    critic.add(Dense(1))\n",
        "\n",
        "    return critic\n",
        "\n",
        "# Gather dataset\n",
        "((X_train, _), (X_test, _)) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Note that I am using test images as fake images for testing purposes\n",
        "interpolated_img = RandomWeightedAverage(32)([X_train[0:32].astype(\"float\"), X_test[32:64].astype(\"float\")])\n",
        "\n",
        "dummy = critic()\n",
        "\n",
        "# Compute gradients of the predictions with respect to the interpolated images\n",
        "with tf.GradientTape() as tape:\n",
        "     y_pred = dummy(interpolated_img)\n",
        "     gradients = tape.gradient(y_pred, interpolated_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKHevkcSERhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfbSRTKqETNk",
        "colab_type": "code",
        "outputId": "49556dd7-f061-4bfa-994f-5c331208804a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#!pip install tensorflow==2.1\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Comes from Generative Deep Learning by David Foster\n",
        "class RandomWeightedAverage(tf.keras.layers.Layer):\n",
        "    def __init__(self, batch_size):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
        "    def call(self, inputs):\n",
        "        alpha = K.random_uniform((self.batch_size, 1, 1, 1))\n",
        "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
        "\n",
        "# Dummy critic\n",
        "def critic():\n",
        "    critic = Sequential()\n",
        "    inputShape = (28, 28, 1)\n",
        "\n",
        "    critic.add(Conv2D(32, (5, 5), padding=\"same\", strides=(2, 2),\n",
        "        input_shape=inputShape))\n",
        "    critic.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    critic.add(Conv2D(64, (5, 5), padding=\"same\", strides=(2, 2)))\n",
        "    critic.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    critic.add(Flatten())\n",
        "    critic.add(Dense(512))\n",
        "    critic.add(LeakyReLU(alpha=0.2))\n",
        "    critic.add(Dropout(0.3))\n",
        "    critic.add(Dense(1))\n",
        "\n",
        "    return critic\n",
        "\n",
        "# Gather dataset\n",
        "((X_train, _), (X_test, _)) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Note that I am using test images as fake images for testing purposes\n",
        "interpolated_img = RandomWeightedAverage(32)([X_train[0:32].astype('float'), X_test[32:64].astype('float')])\n",
        "\n",
        "dummy = critic()\n",
        "\n",
        "# Compute gradients of the predictions with respect to the interpolated images\n",
        "with tf.GradientTape() as tape:\n",
        "    y_pred = dummy(interpolated_img)\n",
        "gradients = tape.gradient(y_pred, interpolated_img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer random_weighted_average_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNyQ36viUI_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(interpolated_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy2JljLJTE3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # (1) Importing dependency\n",
        "    import keras\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "    from keras.layers.normalization import BatchNormalization\n",
        "    import numpy as np\n",
        "    np.random.seed(1000)\n",
        "    \n",
        "    # (2) Get Data\n",
        "    import tflearn.datasets.oxflower17 as oxflower17\n",
        "    x, y = oxflower17.load_data(one_hot=True)\n",
        "    \n",
        "    # (3) Create a sequential model\n",
        "    model = Sequential()\n",
        "    \n",
        "    # 1st Convolutional Layer\n",
        "    model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "    # Batch Normalisation before passing it to the next layer\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 2nd Convolutional Layer\n",
        "    model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 3rd Convolutional Layer\n",
        "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 4th Convolutional Layer\n",
        "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 5th Convolutional Layer\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # Passing it to a dense layer\n",
        "    model.add(Flatten())\n",
        "    # 1st Dense Layer\n",
        "    model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "    model.add(Activation('relu'))\n",
        "    # Add Dropout to prevent overfitting\n",
        "    model.add(Dropout(0.4))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 2nd Dense Layer\n",
        "    model.add(Dense(4096))\n",
        "    model.add(Activation('relu'))\n",
        "    # Add Dropout\n",
        "    model.add(Dropout(0.4))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 3rd Dense Layer\n",
        "    model.add(Dense(1000))\n",
        "    model.add(Activation('relu'))\n",
        "    # Add Dropout\n",
        "    model.add(Dropout(0.4))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # Output Layer\n",
        "    model.add(Dense(17))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    # (4) Compile \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    # (5) Define Gradient Function\n",
        "    def get_gradient_func(model):\n",
        "        grads = K.gradients(model.total_loss, model.trainable_weights)\n",
        "        inputs = model.model._feed_inputs + model.model._feed_targets + model.model._feed_sample_weights\n",
        "        func = K.function(inputs, grads)\n",
        "        return func\n",
        "    \n",
        "    # (6) Train the model such that gradients are captured for every epoch\n",
        "    epoch_gradient = []\n",
        "    for epoch in range(1,5):\n",
        "        model.fit(x, y, batch_size=64, epochs= epoch, initial_epoch = (epoch-1), verbose=1, validation_split=0.2, shuffle=True)\n",
        "        get_gradient = get_gradient_func(model)\n",
        "        grads = get_gradient([x, y, np.ones(len(y))])\n",
        "        #Similarly define your function to play with your model.layers,model.layers[].get_weights(),model.input,model.total_loss,model.trainable_weights etc\n",
        "        # print(\"Layer of the model:\",model.layers[2])\n",
        "        # print(\"Weights of the Layer\",model.layers[2].get_weights())\n",
        "        # print(model.input)\n",
        "        # print(model.total_loss)\n",
        "        # print(model.trainable_weights)\n",
        "        epoch_gradient.append(grads)\n",
        "    \n",
        "    # (7) Convert to a 2 dimensiaonal array of (epoch, gradients) type\n",
        "    gradient = np.asarray(epoch_gradient)\n",
        "    print(\"Total number of epochs run:\", epoch)\n",
        "    print(\"Gradient Array has the shape:\",gradient.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MgmDrsmHkCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (1) Importing dependency\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import numpy as np\n",
        "np.random.seed(1000)\n",
        "\n",
        "# (2) Get Data\n",
        "import tflearn.datasets.oxflower17 as oxflower17\n",
        "x, y = oxflower17.load_data(one_hot=True)\n",
        "\n",
        "# (3) Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation before passing it to the next layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Dense Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 3rd Dense Layer\n",
        "model.add(Dense(1000))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(17))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# (4) Compile \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# (5) Define Gradient Function\n",
        "def get_gradient_func(model):\n",
        "    grads = K.gradients(model.total_loss, model.trainable_weights)\n",
        "    inputs = model.model._feed_inputs + model.model._feed_targets + model.model._feed_sample_weights\n",
        "    func = K.function(inputs, grads)\n",
        "    return func\n",
        "\n",
        "# (6) Train the model such that gradients are captured for every epoch\n",
        "epoch_gradient = []\n",
        "for epoch in range(1,5):\n",
        "    model.fit(x, y, batch_size=64, epochs= epoch, initial_epoch = (epoch-1), verbose=1, validation_split=0.2, shuffle=True)\n",
        "    get_gradient = get_gradient_func(model)\n",
        "    grads = get_gradient([x, y, np.ones(len(y))])\n",
        "    #Similarly define your function to play with your model.layers,model.layers[].get_weights(),model.input,model.total_loss,model.trainable_weights etc\n",
        "    # print(\"Layer of the model:\",model.layers[2])\n",
        "    # print(\"Weights of the Layer\",model.layers[2].get_weights())\n",
        "    # print(model.input)\n",
        "    # print(model.total_loss)\n",
        "    # print(model.trainable_weights)\n",
        "    epoch_gradient.append(grads)\n",
        "\n",
        "# (7) Convert to a 2 dimensiaonal array of (epoch, gradients) type\n",
        "gradient = np.asarray(epoch_gradient)\n",
        "print(\"Total number of epochs run:\", epoch)\n",
        "print(\"Gradient Array has the shape:\",gradient.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyXsTMn1I_oK",
        "colab_type": "code",
        "outputId": "8865e1ea-2c5e-4e85-b417-d1ee78633a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djy7BP1P_V1b",
        "colab_type": "code",
        "outputId": "34a301bc-e0c3-4519-b5cd-f210267823ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# Modify the load image to have the target size\n",
        "img = load_img(\"/Data/dog.jpg\", color_mode=\"grayscale\",interpolation='nearest', target_size=(200,50))\n",
        "\n",
        "# convert to array\n",
        "img = img_to_array(img)\n",
        "print(\"image to array shape:\",img.shape)\n",
        "\n",
        "# reshape into a single sample with 1 channel\n",
        "img = img[np.newaxis,:,:,:]\n",
        "print(\"Add a new axis to specify number of images:\",img.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image to array shape: (200, 50, 1)\n",
            "Add a new axis to specify number of images: (1, 200, 50, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqJyq5h2ArXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FgVvCdgvZBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (1) Importing dependency\n",
        "    import keras\n",
        "    from keras import backend as K\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "    from keras.layers.normalization import BatchNormalization\n",
        "    import numpy as np\n",
        "    np.random.seed(1000)\n",
        "    \n",
        "    # (2) Get Data\n",
        "    import tflearn.datasets.oxflower17 as oxflower17\n",
        "    x, y = oxflower17.load_data(one_hot=True)\n",
        "    \n",
        "    # (3) Create a sequential model\n",
        "    model = Sequential()\n",
        "    \n",
        "    # 1st Convolutional Layer\n",
        "    model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "    # Batch Normalisation before passing it to the next layer\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 2nd Convolutional Layer\n",
        "    model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 3rd Convolutional Layer\n",
        "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 4th Convolutional Layer\n",
        "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 5th Convolutional Layer\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    # Pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # Passing it to a dense layer\n",
        "    model.add(Flatten())\n",
        "    # 1st Dense Layer\n",
        "    model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "    model.add(Activation('relu'))\n",
        "    # Add Dropout to prevent overfitting\n",
        "    model.add(Dropout(0.4))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 2nd Dense Layer\n",
        "    model.add(Dense(4096))\n",
        "    model.add(Activation('relu'))\n",
        "    # Add Dropout\n",
        "    model.add(Dropout(0.4))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # 3rd Dense Layer\n",
        "    model.add(Dense(1000))\n",
        "    model.add(Activation('relu'))\n",
        "    # Add Dropout\n",
        "    model.add(Dropout(0.4))\n",
        "    # Batch Normalisation\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # Output Layer\n",
        "    model.add(Dense(17))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    #model.summary()\n",
        "    \n",
        "    # (4) Compile \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    # (5) Define Gradient Function\n",
        "    def get_gradient_func(model):\n",
        "        grads = K.gradients(model.total_loss, model.trainable_weights)\n",
        "        inputs = model.model._feed_inputs + model.model._feed_targets + model.model._feed_sample_weights\n",
        "        func = K.function(inputs, grads)\n",
        "        return func\n",
        "    \n",
        "    # (6) Train the model such that gradients are captured for every epoch\n",
        "    epoch_gradient = []\n",
        "    for epoch in range(1,5):\n",
        "        model.fit(x, y, batch_size=64, epochs= epoch, initial_epoch = (epoch-1), verbose=1, validation_split=0.2, shuffle=True)\n",
        "        get_gradient = get_gradient_func(model)\n",
        "        grads = get_gradient([x, y, np.ones(len(y))])\n",
        "        # Similarly define your function to play with your model.layers,model.layers[].get_weights(),model.input,model.total_loss,model.trainable_weights etc\n",
        "        # print(\"Layer of the model:\",model.layers[2])\n",
        "        # print(\"Weights of the Layer\",model.layers[2].get_weights())\n",
        "        # print(model.input)\n",
        "        # print(model.total_loss)\n",
        "        # print(model.trainable_weights)\n",
        "        epoch_gradient.append(grads)\n",
        "    \n",
        "    # (7) Convert to a 2 dimensiaonal array of (epoch, gradients) type\n",
        "    gradient = np.asarray(epoch_gradient)\n",
        "    print(\"Total number of epochs run:\", epoch)\n",
        "    print(\"Gradient Array has the shape:\",gradient.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhrY8ER0wGyI",
        "colab_type": "code",
        "outputId": "f03028e0-98be-4db0-e0eb-31b28f3c6ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(gradient.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 34)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4-tOmttwIhJ",
        "colab_type": "code",
        "outputId": "73a9dd1b-5013-4e1a-d77c-6695418b4327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iutilLUp9AsZ",
        "colab_type": "text"
      },
      "source": [
        "# Callbacks in model.fit\n",
        "https://stackoverflow.com/questions/60808723/how-to-call-a-method-as-a-custom-callback-in-keras/60815917#60815917"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK0RorXYQbgM",
        "colab_type": "code",
        "outputId": "20472657-42a6-487d-c6b2-78611cf34291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# (1) Importing dependency\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1000)\n",
        "    \n",
        "# (2) Get Data\n",
        "import tflearn.datasets.oxflower17 as oxflower17\n",
        "x, y = oxflower17.load_data(one_hot=True)\n",
        "    \n",
        "# (3) Create a sequential model\n",
        "model = Sequential()\n",
        "    \n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation before passing it to the next layer\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Dense Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 3rd Dense Layer\n",
        "model.add(Dense(1000))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(17))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# (4) Compile \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "epoch_gradient = []\n",
        "\n",
        "def get_gradient_func(model):\n",
        "    grads = K.gradients(model.total_loss, model.trainable_weights)\n",
        "    inputs = model.model._feed_inputs + model.model._feed_targets + model.model._feed_sample_weights\n",
        "    func = K.function(inputs, grads)\n",
        "    return func\n",
        "\n",
        "# Define the Required Callback Function\n",
        "class GradientCalcCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "      get_gradient = get_gradient_func(model)\n",
        "      grads = get_gradient([x, y, np.ones(len(y))])\n",
        "      epoch_gradient.append(grads)\n",
        "    \n",
        "epoch = 4\n",
        "\n",
        "model.fit(x, y, batch_size=64, epochs= epoch, verbose=1, validation_split=0.2, shuffle=True, callbacks=[GradientCalcCallback()])\n",
        "    \n",
        "# (7) Convert to a 2 dimensiaonal array of (epoch, gradients) type\n",
        "gradient = np.asarray(epoch_gradient)\n",
        "print(\"Total number of epochs run:\", epoch)\n",
        "print(\"Gradient Array has the shape:\",gradient.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 54, 54, 96)        34944     \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 54, 54, 96)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 27, 27, 96)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 27, 27, 96)        384       \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 17, 17, 256)       2973952   \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 17, 17, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 6, 6, 384)         885120    \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 6, 6, 384)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 6, 6, 384)         1536      \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 4, 4, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 4, 4, 384)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 4, 4, 384)         1536      \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 2, 2, 256)         884992    \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 1, 1, 256)         1024      \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 17)                17017     \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 17)                0         \n",
            "=================================================================\n",
            "Total params: 28,096,769\n",
            "Trainable params: 28,075,633\n",
            "Non-trainable params: 21,136\n",
            "_________________________________________________________________\n",
            "Train on 1088 samples, validate on 272 samples\n",
            "Epoch 1/4\n",
            "1088/1088 [==============================] - 5s 5ms/step - loss: 3.0726 - acc: 0.2289 - val_loss: 12.3280 - val_acc: 0.1287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/4\n",
            "1088/1088 [==============================] - 1s 1ms/step - loss: 2.2462 - acc: 0.3327 - val_loss: 7.0050 - val_acc: 0.2500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/4\n",
            "1088/1088 [==============================] - 1s 1ms/step - loss: 1.8286 - acc: 0.4228 - val_loss: 6.0993 - val_acc: 0.2794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/4\n",
            "1088/1088 [==============================] - 1s 1ms/step - loss: 1.6860 - acc: 0.4642 - val_loss: 3.5253 - val_acc: 0.4081\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total number of epochs run: 4\n",
            "Gradient Array has the shape: (4, 34)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCO9AKLKKjgL",
        "colab_type": "code",
        "outputId": "7b85d8b9-cfe0-49cb-8fec-414bc18acef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# summarize filter shapes\n",
        "for layer in model.layers:\n",
        "\t# check for convolutional layer\n",
        "\tif 'conv' in layer.name:\n",
        "\t  # get filter weights\n",
        "\t  filters, biases = layer.get_weights()\n",
        "\t  print(layer.name, filters.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 62\n",
            "conv2d_21 (11, 11, 3, 96)\n",
            "conv2d_22 (11, 11, 96, 256)\n",
            "conv2d_23 (3, 3, 256, 384)\n",
            "conv2d_24 (3, 3, 384, 384)\n",
            "conv2d_25 (3, 3, 384, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmsfcXZtL16-",
        "colab_type": "code",
        "outputId": "2a26d2fc-ce8b-4612-ef85-d043f1a5356c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# summarize filter shapes\n",
        "for layer in model.layers:\n",
        "\t# check for convolutional layer\n",
        "\tif 'conv' in layer.name:\n",
        "\t  # get filter weights\n",
        "\t  filters, biases = layer.get_weights()\n",
        "\n",
        "print(filters.shape)\n",
        "print(filters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 3, 384, 256)\n",
            "[[[[ 1.33229261e-02  2.78362742e-04 -1.15596037e-03 ... -1.04442742e-02\n",
            "    -1.27202040e-02  2.72896886e-02]\n",
            "   [ 2.04270035e-02  2.09266972e-02 -2.27798354e-02 ...  2.10328754e-02\n",
            "    -1.79852340e-02 -1.31000848e-02]\n",
            "   [ 1.21402023e-02  9.55937244e-03 -9.73658753e-04 ...  2.90534608e-02\n",
            "     8.62147380e-03  3.97700304e-03]\n",
            "   ...\n",
            "   [ 2.05181465e-02 -2.70775743e-02 -1.03279278e-02 ...  4.66628186e-03\n",
            "     2.88294349e-02  3.55665796e-02]\n",
            "   [ 2.56488305e-02  2.02665925e-02 -9.53960046e-03 ...  1.07415328e-02\n",
            "     2.38317680e-02 -1.13734556e-02]\n",
            "   [-6.18373556e-03  7.84027111e-03  4.98771551e-04 ...  1.88040324e-02\n",
            "    -2.21270267e-02  7.43205333e-03]]\n",
            "\n",
            "  [[-3.35427485e-02  2.35256236e-02 -1.55152555e-03 ... -1.82591029e-03\n",
            "    -6.07969100e-03 -2.40267627e-03]\n",
            "   [-5.59751503e-03  3.25453505e-02 -1.27216885e-02 ...  2.22896207e-02\n",
            "    -1.04518968e-03 -3.41729373e-02]\n",
            "   [-1.53387915e-02 -2.15985011e-02 -2.40802132e-02 ...  8.57070833e-03\n",
            "     1.55310007e-02  8.27453937e-03]\n",
            "   ...\n",
            "   [-1.83913186e-02 -2.47003324e-02 -1.77569594e-02 ...  3.14827859e-02\n",
            "     2.37260144e-02  1.35977212e-02]\n",
            "   [-5.08161262e-03  2.33772658e-02 -1.85138639e-02 ... -1.16026518e-03\n",
            "     2.38104146e-02 -1.90851931e-02]\n",
            "   [ 8.57734215e-03 -1.23164626e-02 -1.81618647e-03 ... -1.26188034e-02\n",
            "     2.13982817e-02  1.71989426e-02]]\n",
            "\n",
            "  [[ 2.75964546e-03  4.13805023e-02 -1.22120418e-02 ...  1.71306508e-03\n",
            "    -1.51561396e-02 -2.18702871e-02]\n",
            "   [-3.56088951e-03  3.43757658e-03 -1.16327219e-02 ... -1.19192926e-02\n",
            "     3.88762157e-04 -9.95170232e-03]\n",
            "   [ 3.59454565e-02 -6.90977415e-03 -1.50266113e-02 ... -3.32049071e-03\n",
            "     2.45900005e-02 -2.42324471e-02]\n",
            "   ...\n",
            "   [-1.90636870e-02 -1.24915438e-02  2.68660672e-02 ... -4.73172823e-03\n",
            "    -5.91639895e-03  2.28889082e-02]\n",
            "   [ 3.66363376e-02 -2.24502403e-02 -1.06375702e-02 ... -1.81130767e-02\n",
            "     2.88566016e-02 -2.08211727e-02]\n",
            "   [ 6.01422449e-04 -8.68872181e-03 -2.10836846e-02 ...  1.67874563e-02\n",
            "    -3.84189188e-02  1.78155117e-02]]]\n",
            "\n",
            "\n",
            " [[[ 1.22205978e-02  4.27871849e-03 -1.09987976e-02 ...  1.76646598e-02\n",
            "     1.50742009e-02 -3.86701301e-02]\n",
            "   [-2.79729012e-02  2.00011078e-02 -1.31703299e-02 ... -2.00200267e-02\n",
            "    -7.25999195e-03  1.45357484e-02]\n",
            "   [-4.82261041e-03 -4.14834311e-03 -1.76655632e-02 ...  2.20327824e-02\n",
            "     1.72639880e-02 -7.53242476e-03]\n",
            "   ...\n",
            "   [ 1.73690319e-02  3.01635619e-02  1.45886336e-02 ...  1.34067582e-02\n",
            "     3.23755704e-02  1.59694552e-02]\n",
            "   [ 2.47067846e-02  7.35560572e-03 -1.88481305e-02 ...  2.58524362e-02\n",
            "     9.14169010e-04  2.94958130e-02]\n",
            "   [ 2.67407298e-02 -6.96273427e-03  2.19189376e-02 ... -3.23407352e-02\n",
            "     1.11467652e-02 -2.02568714e-02]]\n",
            "\n",
            "  [[-3.33763249e-02 -1.54417353e-02 -8.87046568e-03 ...  1.41540635e-02\n",
            "    -2.68381871e-02  1.22759230e-02]\n",
            "   [-1.30444350e-05 -7.20519293e-03  1.52943395e-02 ... -1.05463304e-02\n",
            "    -1.63246077e-02 -2.80507226e-02]\n",
            "   [ 1.94950942e-02  1.96460653e-02 -5.00125624e-03 ... -1.58436447e-02\n",
            "     1.41756414e-02  1.98035198e-03]\n",
            "   ...\n",
            "   [-2.91450340e-02  1.30721824e-02  1.83877610e-02 ... -1.90879926e-02\n",
            "     1.99749433e-02 -1.05333803e-02]\n",
            "   [ 1.72556620e-02  1.81617010e-02  1.24600148e-02 ...  1.88519955e-02\n",
            "    -8.84543266e-03 -1.60351787e-02]\n",
            "   [ 2.47481037e-02  2.93982169e-03  2.01242752e-02 ...  1.95950232e-02\n",
            "    -2.78105512e-02  2.05665398e-02]]\n",
            "\n",
            "  [[ 1.82851534e-02  3.44641991e-02 -2.00678073e-02 ... -3.24464031e-02\n",
            "    -3.23583074e-02 -2.66336314e-02]\n",
            "   [-1.30740115e-02 -2.40531545e-02  1.92862866e-03 ... -9.58005898e-03\n",
            "    -8.63340776e-03 -2.82395743e-02]\n",
            "   [ 4.19791276e-03  1.45691829e-02  3.42715830e-02 ...  4.06611152e-02\n",
            "    -2.05075163e-02 -3.97980548e-02]\n",
            "   ...\n",
            "   [ 1.23621235e-02 -3.30808535e-02  3.05603142e-03 ... -8.03769939e-03\n",
            "     2.39255354e-02  1.01700425e-02]\n",
            "   [ 1.93574950e-02  2.53207628e-02  3.19945849e-02 ... -1.29298661e-02\n",
            "    -9.71809588e-03 -3.35334092e-02]\n",
            "   [-4.02106047e-02 -2.33742259e-02 -2.70008687e-02 ... -5.98855503e-03\n",
            "    -1.24435341e-02  3.15479822e-02]]]\n",
            "\n",
            "\n",
            " [[[-2.30802242e-02 -1.40459323e-02 -6.23126328e-03 ...  1.52085479e-02\n",
            "    -2.52160486e-02  1.24641499e-02]\n",
            "   [ 1.96760800e-02  1.05630013e-03  2.54229661e-02 ...  1.62760932e-02\n",
            "     2.23252065e-02 -2.22447589e-02]\n",
            "   [ 3.04492004e-02 -1.94075168e-03 -1.97871421e-02 ... -9.06596240e-03\n",
            "     3.24702673e-02 -9.38400440e-03]\n",
            "   ...\n",
            "   [-2.87375692e-02 -1.13275591e-02 -1.56598929e-02 ... -3.06045953e-02\n",
            "    -3.27123553e-02  4.29812027e-03]\n",
            "   [-5.72555349e-04  9.24452208e-03 -1.79962069e-02 ... -1.38692930e-03\n",
            "    -1.69475167e-03 -2.07326356e-02]\n",
            "   [ 2.55298633e-02  2.39779558e-02 -4.83227056e-03 ... -1.02796387e-02\n",
            "    -1.41589378e-04 -1.67643074e-02]]\n",
            "\n",
            "  [[-2.23147380e-03  1.37838507e-02 -4.08266764e-03 ...  1.90702491e-02\n",
            "     9.72075947e-03 -2.66824216e-02]\n",
            "   [-1.86436921e-02 -2.82567251e-03 -1.35208173e-02 ...  3.81241441e-02\n",
            "    -1.62060019e-02 -3.96039486e-02]\n",
            "   [ 5.92509052e-03  1.42326870e-03  5.25515201e-03 ...  2.07302850e-02\n",
            "    -3.06541529e-02 -2.26684157e-02]\n",
            "   ...\n",
            "   [ 2.35446766e-02 -3.54932398e-02 -1.09605016e-02 ...  1.20483274e-02\n",
            "     3.52192447e-02 -8.96899775e-03]\n",
            "   [-1.91081129e-02 -7.72378687e-03  8.51210579e-03 ...  3.27213518e-02\n",
            "    -3.99521226e-03  2.70088650e-02]\n",
            "   [-1.36597576e-02  7.93528836e-03  4.57486464e-03 ... -3.79467532e-02\n",
            "    -1.68086924e-02  3.47109861e-03]]\n",
            "\n",
            "  [[-2.51405891e-02  3.37613821e-02 -1.24929231e-02 ... -4.84925555e-03\n",
            "     1.82562377e-02 -4.42078747e-02]\n",
            "   [-5.84426289e-03 -2.04253178e-02  1.42424544e-02 ...  2.39489395e-02\n",
            "    -3.39500532e-02 -7.58632645e-03]\n",
            "   [ 2.52021682e-02 -2.40255445e-02 -1.36912316e-02 ... -1.96786743e-04\n",
            "     7.05324765e-03 -1.00541944e-02]\n",
            "   ...\n",
            "   [ 9.71514825e-03 -7.74992211e-03 -3.24081630e-02 ...  1.33663723e-02\n",
            "     1.77465230e-02  2.45424472e-02]\n",
            "   [-1.41809247e-02  2.96245646e-02  3.87504548e-02 ...  1.54969441e-02\n",
            "    -2.57334486e-02 -4.18210924e-02]\n",
            "   [-7.17054168e-03  3.36966068e-02 -2.70571802e-02 ... -3.91547196e-03\n",
            "    -1.72355976e-02  2.01625824e-02]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjwfJo493IAV",
        "colab_type": "text"
      },
      "source": [
        "# Tensor to Array(ndarray) \n",
        "https://stackoverflow.com/questions/60824788/how-to-convert-tensor-to-ndarray"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9cwPNdunwz7",
        "colab_type": "code",
        "outputId": "e6b65ff5-5de2-4eb5-d06b-cb19c5497844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#!pip install tensorflow==2.1\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#import time\n",
        "\n",
        "print(\"tensorflow version:\",tf.__version__)\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(7,(3,3) , padding = \"same\" , input_shape = (28,28,1)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Dense(50,activation = 'relu'))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    return model \n",
        "\n",
        "model_discriminator = make_discriminator_model()\n",
        "output = model_discriminator(np.random.rand(1,28,28,1).astype(\"float32\"))\n",
        "print(\"Output as a Tensor:\",output)\n",
        "\n",
        "out = np.array(output)\n",
        "print(\"Output as an Array:\",out)\n",
        "print(\"Type of the Array:\",type(out))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow version: 2.1.0\n",
            "Output as a Tensor: tf.Tensor([[-0.40550372]], shape=(1, 1), dtype=float32)\n",
            "Output as an Array: [[-0.40550372]]\n",
            "Type of the Array: <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVHp5zzm0jbd",
        "colab_type": "code",
        "outputId": "7e589da7-084f-4643-8810-5e40c4102e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#!pip install tensorflow==1.14\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "print(\"tensorflow version:\",tf.__version__)\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(7,(3,3) , padding = \"same\" , input_shape = (28,28,1)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Dense(50,activation = 'relu'))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    return model \n",
        "\n",
        "model_discriminator = make_discriminator_model()\n",
        "output = model_discriminator(np.random.rand(1,28,28,1).astype(\"float32\"))\n",
        "\n",
        "#initialize the variable\n",
        "init_op = tf.initialize_all_variables()\n",
        "\n",
        "#run the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init_op) #execute init_op\n",
        "    print(\"Output as a Tensor:\",output)\n",
        "    out = np.array(sess.run(output))\n",
        "    print(\"Output as an Array:\",out)\n",
        "    print(\"Type of the Array:\",type(out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow version: 1.14.0\n",
            "Output as a Tensor: Tensor(\"sequential_7/dense_15/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
            "Output as an Array: [[-0.29746282]]\n",
            "Type of the Array: <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEyyro-xMnwS",
        "colab_type": "text"
      },
      "source": [
        "# Switching between Tensorflow Versions without installing everytime\n",
        "https://stackoverflow.com/questions/60810400/how-to-upgrade-tensorflow-to-2-0-in-google-colab-permanently/60810715#60810715"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PKQyncnMmBa",
        "colab_type": "code",
        "outputId": "3e556b13-75f0-4eaf-a708-94638c6e8099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVg8VdqXNHeN",
        "colab_type": "code",
        "outputId": "5f1425f1-d300-47b4-8db3-7e3e900b2d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f2C617e-BFu",
        "colab_type": "text"
      },
      "source": [
        "# One Hot Encoding Using LabelBinarizer\n",
        "https://stackoverflow.com/questions/60868391/how-to-view-class-labels-after-one-hot-encoding-during-training-testing-and-afte/60871869#60871869"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsvwViobiReg",
        "colab_type": "code",
        "outputId": "52777050-e087-4be8-e5c1-aefd120f0c12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# define example\n",
        "data = ['dog', 'dog', 'cat', 'dog', 'cat', 'cat', 'dog', 'cat', 'dog', 'dog']\n",
        "\n",
        "values = np.array(data)\n",
        "\n",
        "#Binary encode\n",
        "lb = LabelBinarizer()\n",
        "\n",
        "labels = lb.fit_transform(values)\n",
        "labels = to_categorical(labels)\n",
        "print(\"which position represents for cat and dog?:\")\n",
        "print(\"Data is:\",data)\n",
        "print(labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "which position represents for cat and dog?:\n",
            "Data is: ['dog', 'dog', 'cat', 'dog', 'cat', 'cat', 'dog', 'cat', 'dog', 'dog']\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ4RkAQAKsje",
        "colab_type": "code",
        "outputId": "6ccc26e1-0cbb-41e8-9c73-5ff4ac30442e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# define example\n",
        "data1 = ['cat', 'cat', 'dog', 'cat', 'dog', 'dog', 'cat', 'dog', 'cat', 'cat']\n",
        "data2 = ['dog', 'dog', 'cat', 'dog', 'cat', 'cat', 'dog', 'cat', 'dog', 'dog']\n",
        "\n",
        "values1 = np.array(data1)\n",
        "values2 = np.array(data2)\n",
        "\n",
        "#Binary encode\n",
        "lb = LabelBinarizer()\n",
        "\n",
        "labels1 = lb.fit_transform(values1)\n",
        "labels1 = to_categorical(labels1)\n",
        "print(\"what is value for cat and dog?:\")\n",
        "print(\"Data is:\",data1)\n",
        "print(labels1)\n",
        "print(\"\\n\")\n",
        "\n",
        "labels2 = lb.fit_transform(values2)\n",
        "labels2 = to_categorical(labels2)\n",
        "print(\"what is value for cat and dog?:\")\n",
        "print(\"Data is:\",data2)\n",
        "print(labels2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what is value for cat and dog?:\n",
            "Data is: ['cat', 'cat', 'dog', 'cat', 'dog', 'dog', 'cat', 'dog', 'cat', 'cat']\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "\n",
            "\n",
            "what is value for cat and dog?:\n",
            "Data is: ['dog', 'dog', 'cat', 'dog', 'cat', 'cat', 'dog', 'cat', 'dog', 'dog']\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFIx6yucLI-K",
        "colab_type": "code",
        "outputId": "d99da379-cfe0-48ec-9e2e-41f6c23a10e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "rslt = np.array([[0.9550967,0.04490325]])\n",
        "rslt = np.argmax(rslt)\n",
        "print(rslt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIZpkS8Sh223",
        "colab_type": "code",
        "outputId": "0e98fc0b-6ee4-4012-fc7d-3b1563c3026e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "rslt = np.array([[0.04490325,0.9550967, 1]])\n",
        "rslt = np.argmax(rslt)\n",
        "print(rslt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_jgkP3sdF4r",
        "colab_type": "text"
      },
      "source": [
        "# Ragged Tensor\n",
        "https://stackoverflow.com/questions/60924624/is-there-a-way-to-normalize-a-ragged-tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dqzlnvmwrr3",
        "colab_type": "text"
      },
      "source": [
        "## Using math.l2_normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REq-0WuSdgT3",
        "colab_type": "code",
        "outputId": "142206b5-c025-4ce7-ee90-028d2ecc380b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Create a Ragged Tensor\n",
        "rt = tf.ragged.constant([[9.0, 8.0, 7.0], [], [6.0, 5.0], [4.0]])\n",
        "print(\"Ragged Tensor:\",\"\\n\",rt,\"\\n\")\n",
        "\n",
        "# Convert to Tensor to have same length\n",
        "rt = rt.to_tensor()\n",
        "print(\"Tensor of same length:\",\"\\n\",rt,\"\\n\")\n",
        "\n",
        "# Normalize\n",
        "rt = tf.math.l2_normalize(rt, axis = None)\n",
        "print(\"Normalized Tensor:\",\"\\n\",rt,\"\\n\")\n",
        "\n",
        "# Convert to Ragged Tensor\n",
        "rt = tf.RaggedTensor.from_tensor(rt, padding=0.0)\n",
        "print(\"Normalized Ragged Tensor:\",\"\\n\",rt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ragged Tensor: \n",
            " <tf.RaggedTensor [[9.0, 8.0, 7.0], [], [6.0, 5.0], [4.0]]> \n",
            "\n",
            "Tensor of same length: \n",
            " tf.Tensor(\n",
            "[[9. 8. 7.]\n",
            " [0. 0. 0.]\n",
            " [6. 5. 0.]\n",
            " [4. 0. 0.]], shape=(4, 3), dtype=float32) \n",
            "\n",
            "Normalized Tensor: \n",
            " tf.Tensor(\n",
            "[[0.546711   0.48596537 0.4252197 ]\n",
            " [0.         0.         0.        ]\n",
            " [0.36447403 0.30372834 0.        ]\n",
            " [0.24298269 0.         0.        ]], shape=(4, 3), dtype=float32) \n",
            "\n",
            "Normalized Ragged Tensor: \n",
            " <tf.RaggedTensor [[0.5467110276222229, 0.485965371131897, 0.42521971464157104], [], [0.36447402834892273, 0.3037283420562744], [0.2429826855659485]]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_FhnDE9wzlF",
        "colab_type": "text"
      },
      "source": [
        "## Using tf.linalg.normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzHoyROQdpHI",
        "colab_type": "code",
        "outputId": "f16f1518-fc9a-4ed0-83aa-2f3f78f813d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Create a Ragged Tensor\n",
        "rt = tf.ragged.constant([[9.0, 8.0, 7.0], [], [6.0, 5.0], [4.0]])\n",
        "print(\"Ragged Tensor:\",\"\\n\",rt,\"\\n\")\n",
        "\n",
        "# Convert to Tensor to have same length\n",
        "rt = rt.to_tensor()\n",
        "print(\"Tensor of same length:\",\"\\n\",rt,\"\\n\")\n",
        "\n",
        "# Normalize\n",
        "rt = tf.linalg.normalize(rt, axis = None)\n",
        "print(\"Normalized and Norm Tensor:\",\"\\n\",rt,\"\\n\")\n",
        "# Get the normalized part\n",
        "rt = tf.convert_to_tensor(rt[0])\n",
        "print(\"Normalized Tensor:\",\"\\n\",rt,\"\\n\")\n",
        "\n",
        "# Convert to Ragged Tensor\n",
        "rt = tf.RaggedTensor.from_tensor(rt, padding=0.0)\n",
        "print(\"Normalized Ragged Tensor:\",\"\\n\",rt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ragged Tensor: \n",
            " tf.RaggedTensor(values=Tensor(\"RaggedConstant/values:0\", shape=(6,), dtype=float32), row_splits=Tensor(\"RaggedConstant/Const:0\", shape=(5,), dtype=int64)) \n",
            "\n",
            "Tensor of same length: \n",
            " Tensor(\"RaggedToTensor/GatherV2:0\", shape=(4, 3), dtype=float32) \n",
            "\n",
            "Normalized and Norm Tensor: \n",
            " (<tf.Tensor 'normalize/truediv:0' shape=(4, 3) dtype=float32>, <tf.Tensor 'normalize/norm/Sqrt:0' shape=(1, 1) dtype=float32>) \n",
            "\n",
            "Normalized Tensor: \n",
            " Tensor(\"normalize/truediv:0\", shape=(4, 3), dtype=float32) \n",
            "\n",
            "Normalized Ragged Tensor: \n",
            " tf.RaggedTensor(values=Tensor(\"RaggedFromTensor/boolean_mask/GatherV2:0\", shape=(?,), dtype=float32), row_splits=Tensor(\"RaggedFromTensor/concat:0\", shape=(5,), dtype=int64))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVLkvAkPatDD",
        "colab_type": "text"
      },
      "source": [
        "# Deleting Layer using Keras Surgeon OR pop\n",
        "https://stackoverflow.com/questions/60637199/error-in-removing-the-first-layer-of-keras-model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb4cJKwt2eIK",
        "colab_type": "text"
      },
      "source": [
        "## Deleting the first or middle layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcVNcMX4eV1A",
        "colab_type": "code",
        "outputId": "94ffb433-76fb-4c11-9c99-b449fd298fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install kerassurgeon"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kerassurgeon\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/e7/8adbef95f56e2349bf9faf2aec462dee0a38cec7cd6bfb8895de83706762/kerassurgeon-0.1.3-py3-none-any.whl\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from kerassurgeon) (2.2.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->kerassurgeon) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->kerassurgeon) (1.18.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->kerassurgeon) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->kerassurgeon) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->kerassurgeon) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->kerassurgeon) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->kerassurgeon) (3.13)\n",
            "Installing collected packages: kerassurgeon\n",
            "Successfully installed kerassurgeon-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3d88fe8b-4d7c-4c16-9294-e641eefb5d1a",
        "id": "K9OBGZECaruM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "# (1) Importing dependency\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, Input\n",
        "\n",
        "import kerassurgeon\n",
        "from kerassurgeon.operations import delete_layer, insert_layer, delete_channels\n",
        "    \n",
        "# (3) Create a sequential model\n",
        "model = Sequential()\n",
        "    \n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=12, input_shape=(24,24,1), kernel_size=(3,3), activation='relu'))\n",
        " \n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=24, kernel_size=(3,3), activation='relu'))\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=24, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense Layer\n",
        "model.add(Dense(5,activation='relu'))\n",
        "\n",
        "# Dense Layer\n",
        "model.add(Dense(100,activation='relu'))\n",
        "\n",
        "# Dense Layer\n",
        "model.add(Dense(100,activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# delete 3rd layer .i.e. Conv2D Layer from the model\n",
        "layer_3 = model.layers[2]\n",
        "model = delete_layer(model, layer_3)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 22, 22, 12)        120       \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 20, 20, 24)        2616      \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 20, 20, 24)        5208      \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 9600)              0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 5)                 48005     \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 100)               600       \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 66,952\n",
            "Trainable params: 66,952\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19_input (InputLayer) (None, 24, 24, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 22, 22, 12)        120       \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 20, 20, 24)        2616      \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 9600)              0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 5)                 48005     \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 100)               600       \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 61,744\n",
            "Trainable params: 61,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJidi2xObYul",
        "colab_type": "code",
        "outputId": "ad1db41b-aa86-4bd1-f0c5-8cf2f485e1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "# (1) Importing dependency\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, Input\n",
        "\n",
        "import kerassurgeon\n",
        "from kerassurgeon import Surgeon\n",
        "    \n",
        "# (3) Create a sequential model\n",
        "model = Sequential()\n",
        "    \n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=12, input_shape=(24,24,1), kernel_size=(3,3), activation='relu'))\n",
        " \n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=24, kernel_size=(3,3), activation='relu'))\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=24, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense Layer\n",
        "model.add(Dense(5,activation='relu'))\n",
        "\n",
        "# Dense Layer\n",
        "model.add(Dense(100,activation='relu'))\n",
        "\n",
        "# Dense Layer\n",
        "model.add(Dense(100,activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# delete 3rd layer .i.e. Conv2D Layer from the model\n",
        "layer_3 = model.layers[2]\n",
        "surgeon = Surgeon(model)\n",
        "surgeon.add_job('delete_layer', layer_3)\n",
        "model = surgeon.operate()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 22, 22, 12)        120       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 20, 20, 24)        2616      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 20, 20, 24)        5208      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 9600)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 5)                 48005     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               600       \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 66,952\n",
            "Trainable params: 66,952\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-79f9b15a709b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0msurgeon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSurgeon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0msurgeon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delete_layer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurgeon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36moperate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_inbound_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mnew_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m_rebuild_graph\u001b[0;34m(self, graph_inputs, output_nodes, graph_input_masks)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# Call the recursive _rebuild_rec method to rebuild the submodel up to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# each output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_rebuild_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# Call the recursive _rebuild_rec method to rebuild the submodel up to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# each output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_rebuild_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m_rebuild_rec\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m_rebuild_rec\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m_rebuild_rec\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m_rebuild_rec\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m_rebuild_rec\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m_rebuild_rec\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# obtain its inputs and input masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 inputs, input_masks = zip(\n\u001b[0;32m--> 240\u001b[0;31m                     *[_rebuild_rec(n) for n in inbound_nodes])\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerassurgeon/surgeon.py\u001b[0m in \u001b[0;36m_rebuild_rec\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                     \u001b[0mnew_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_delete_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;31m# Record that this node has been rebuild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# with the input_spec set at build time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;31m# Handle mask propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    344\u001b[0m                                 \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' of input shape to have '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                                 \u001b[0;34m'value '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                                 ' but got shape ' + str(x_shape))\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;31m# Check shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_2: expected axis -1 of input shape to have value 12 but got shape (None, 24, 24, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c9f5b5a2-e93f-483d-c421-12e7b94213b6",
        "id": "GHOS2dQEvPkn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# (1) Importing dependency\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import numpy as np\n",
        "    \n",
        "# (3) Create a sequential model\n",
        "model = Sequential()\n",
        "    \n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation before passing it to the next layer\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Dense Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 3rd Dense Layer\n",
        "model.add(Dense(1000))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(17))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model1 = model.layers.pop(0)\n",
        "\n",
        "model1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_61 (Conv2D)           (None, 54, 54, 96)        34944     \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 54, 54, 96)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 27, 27, 96)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 27, 27, 96)        384       \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 17, 17, 256)       2973952   \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 17, 17, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_63 (Conv2D)           (None, 6, 6, 384)         885120    \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 6, 6, 384)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 6, 6, 384)         1536      \n",
            "_________________________________________________________________\n",
            "conv2d_64 (Conv2D)           (None, 4, 4, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 4, 4, 384)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 4, 4, 384)         1536      \n",
            "_________________________________________________________________\n",
            "conv2d_65 (Conv2D)           (None, 2, 2, 256)         884992    \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 1, 1, 256)         1024      \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 17)                17017     \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 17)                0         \n",
            "=================================================================\n",
            "Total params: 28,096,769\n",
            "Trainable params: 28,075,633\n",
            "Non-trainable params: 21,136\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d9f744fc3690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Conv2D' object has no attribute 'summary'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_5aAAf82LIE",
        "colab_type": "text"
      },
      "source": [
        "## To remove the last layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "aabcfd59-f6e6-4940-9749-cf6667dccbf5",
        "id": "JU19GZBdx4eq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "# (1) Importing dependency\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, Input\n",
        "\n",
        "import kerassurgeon\n",
        "from kerassurgeon import Surgeon\n",
        "    \n",
        "# (3) Create a sequential model\n",
        "model = Sequential()\n",
        "    \n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=12, input_shape=(24,24,1), kernel_size=(3,3), activation='relu'))\n",
        " \n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=24, kernel_size=(3,3), activation='relu'))\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=24, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense Layer\n",
        "model.add(Dense(5,activation='relu'))\n",
        "\n",
        "# Dense Layer\n",
        "model.add(Dense(100,activation='relu'))\n",
        "\n",
        "# Dense Layer\n",
        "model.add(Dense(100,activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "from keras.models import Model\n",
        "\n",
        "model._layers.pop()\n",
        "\n",
        "new_model = Model(model.input,model.layers[-1].output)\n",
        "\n",
        "new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_43\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_138 (Conv2D)          (None, 22, 22, 12)        120       \n",
            "_________________________________________________________________\n",
            "conv2d_139 (Conv2D)          (None, 20, 20, 24)        2616      \n",
            "_________________________________________________________________\n",
            "conv2d_140 (Conv2D)          (None, 20, 20, 24)        5208      \n",
            "_________________________________________________________________\n",
            "flatten_42 (Flatten)         (None, 9600)              0         \n",
            "_________________________________________________________________\n",
            "dense_165 (Dense)            (None, 5)                 48005     \n",
            "_________________________________________________________________\n",
            "dense_166 (Dense)            (None, 100)               600       \n",
            "_________________________________________________________________\n",
            "dense_167 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_168 (Dense)            (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 66,952\n",
            "Trainable params: 66,952\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_138_input (InputLayer (None, 24, 24, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_138 (Conv2D)          (None, 22, 22, 12)        120       \n",
            "_________________________________________________________________\n",
            "conv2d_139 (Conv2D)          (None, 20, 20, 24)        2616      \n",
            "_________________________________________________________________\n",
            "conv2d_140 (Conv2D)          (None, 20, 20, 24)        5208      \n",
            "_________________________________________________________________\n",
            "flatten_42 (Flatten)         (None, 9600)              0         \n",
            "_________________________________________________________________\n",
            "dense_165 (Dense)            (None, 5)                 48005     \n",
            "_________________________________________________________________\n",
            "dense_166 (Dense)            (None, 100)               600       \n",
            "_________________________________________________________________\n",
            "dense_167 (Dense)            (None, 100)               10100     \n",
            "=================================================================\n",
            "Total params: 66,649\n",
            "Trainable params: 66,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iah4GD0dbRBW",
        "colab_type": "text"
      },
      "source": [
        "# Multiple image input for keras application\n",
        "https://stackoverflow.com/questions/60582442/multiple-image-input-for-keras-application/60968842#60968842"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN-AY-jDdpLV",
        "colab_type": "code",
        "outputId": "9ab40775-483d-48cb-d651-e1d7b890a4eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.python.keras import layers, models, applications\n",
        "\n",
        "# Multiple inputs\n",
        "in1 = layers.Input(shape=(128,128,3))\n",
        "in2 = layers.Input(shape=(128,128,3))\n",
        "in3 = layers.Input(shape=(128,128,3))\n",
        "\n",
        "# CNN output\n",
        "cnn = applications.xception.Xception(include_top=False)\n",
        "cnn.summary()\n",
        "\n",
        "out1 = cnn(in1)\n",
        "out2 = cnn(in2)\n",
        "out3 = cnn(in3)\n",
        "\n",
        "# Flattening the output for the dense layer\n",
        "fout1 = layers.Flatten()(out1)\n",
        "fout2 = layers.Flatten()(out2)\n",
        "fout3 = layers.Flatten()(out3)\n",
        "\n",
        "# Getting the dense output\n",
        "dense = layers.Dense(100, activation='softmax')\n",
        "\n",
        "dout1 = dense(fout1)\n",
        "dout2 = dense(fout2)\n",
        "dout3 = dense(fout3)\n",
        "\n",
        "# Concatenating the final output\n",
        "out = layers.Concatenate(axis=-1)([dout1, dout2, dout3])\n",
        "\n",
        "# Creating the model\n",
        "model = models.Model(inputs=[in1,in2,in3], outputs=out)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"xception\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, None, None,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, None, None, 3 864         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, None, None, 3 128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, None, None, 3 0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, None, None, 6 18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, None, None, 6 256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, None, None, 6 0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, None, None, 1 8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, None, None, 1 512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, None, None, 1 0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, None, None, 1 17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, None, None, 1 512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, None, None, 1 8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, None, None, 1 0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, None, None, 1 512         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, None, None, 1 0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, None, None, 1 0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, None, None, 2 33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, None, None, 2 1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, None, None, 2 0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, None, None, 2 67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, None, None, 2 1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, None, None, 2 32768       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, None, None, 2 0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, None, 2 1024        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, None, None, 2 0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, None, None, 2 0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, None, None, 7 188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, None, None, 7 2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, None, None, 7 0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, None, None, 7 536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, None, None, 7 2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, None, None, 7 186368      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, None, None, 7 0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, None, 7 2912        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, None, None, 7 0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, None, None, 7 0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, None, None, 7 536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, None, None, 7 2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, None, None, 7 0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, None, None, 7 536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, None, None, 7 2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, None, None, 7 0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, None, None, 7 536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, None, None, 7 2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, None, None, 7 0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, None, None, 7 0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, None, None, 7 536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, None, None, 7 2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, None, None, 7 0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, None, None, 7 536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, None, None, 7 2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, None, None, 7 0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, None, None, 7 536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, None, None, 7 2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, None, None, 7 0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, None, None, 7 0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, None, None, 7 536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, None, None, 7 2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, None, None, 7 0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, None, None, 7 536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, None, None, 7 2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, None, None, 7 0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, None, None, 7 536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, None, None, 7 2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, None, None, 7 0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, None, None, 7 0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, None, None, 7 536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, None, None, 7 2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, None, None, 7 0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, None, None, 7 536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, None, None, 7 2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, None, None, 7 0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, None, None, 7 536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, None, None, 7 2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, None, None, 7 0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, None, None, 7 0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, None, None, 7 536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, None, None, 7 2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, None, None, 7 0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, None, None, 7 536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, None, None, 7 2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, None, None, 7 0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, None, None, 7 536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, None, None, 7 2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, None, None, 7 0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, None, None, 7 0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, None, None, 7 536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, None, None, 7 2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, None, None, 7 0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, None, None, 7 536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, None, None, 7 2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, None, None, 7 0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, None, None, 7 536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, None, None, 7 2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, None, None, 7 0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, None, None, 7 0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, None, None, 7 536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, None, None, 7 2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, None, None, 7 0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, None, None, 7 536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, None, None, 7 2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, None, None, 7 0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, None, None, 7 536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, None, None, 7 2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, None, None, 7 0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, None, None, 7 0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, None, None, 7 536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, None, None, 7 2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, None, None, 7 0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, None, None, 7 536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, None, None, 7 2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, None, None, 7 0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, None, None, 7 536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, None, None, 7 2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, None, None, 7 0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, None, None, 7 0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, None, None, 7 536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, None, None, 7 2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, None, None, 7 0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, None, None, 1 752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, None, None, 1 4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, None, None, 1 745472      add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, None, None, 1 0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, None, 1 4096        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, None, None, 1 0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, None, None, 1 1582080     add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, None, None, 1 6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, None, None, 1 0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, None, None, 2 3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, None, None, 2 8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, None, None, 2 0           block14_sepconv2_bn[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 20,861,480\n",
            "Trainable params: 20,806,952\n",
            "Non-trainable params: 54,528\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "xception (Model)                multiple             20861480    input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 32768)        0           xception[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 32768)        0           xception[2][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 32768)        0           xception[3][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          3276900     flatten[0][0]                    \n",
            "                                                                 flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 300)          0           dense[0][0]                      \n",
            "                                                                 dense[1][0]                      \n",
            "                                                                 dense[2][0]                      \n",
            "==================================================================================================\n",
            "Total params: 24,138,380\n",
            "Trainable params: 24,083,852\n",
            "Non-trainable params: 54,528\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsDMeCKnrbCJ",
        "colab_type": "code",
        "outputId": "b34b9515-b774-4e25-9bf3-be20aff29de3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "import keras\n",
        "from keras import Input, Model\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.layers import Dense\n",
        "from keras.activations import relu\n",
        "\n",
        "input_shape = (32,32,3)\n",
        "#[(None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3)]\n",
        "#rt = tf.ragged.constant([[9.0, 8.0, 7.0], [], [6.0, 5.0], [4.0]])\n",
        "\n",
        "in1 = Input(shape=(32,32,3))\n",
        "in2 = Input(shape=(32,32,3))\n",
        "in3 = Input(shape=(32,32,3))\n",
        "in4 = Input(shape=(32,32,3))\n",
        "in5 = Input(shape=(32,32,3))\n",
        "in6 = Input(shape=(32,32,3))\n",
        "in7 = Input(shape=(32,32,3))\n",
        "in8 = Input(shape=(32,32,3))\n",
        "in9 = Input(shape=(32,32,3))\n",
        "in10 = Input(shape=(32,32,3))\n",
        "in11 = Input(shape=(32,32,3))\n",
        "\n",
        "inputs = [in1,in2,in3,in4,in5,in6,in7,in8,in9,in10,in11]\n",
        "densenet_121_model = DenseNet121(include_top=False)(inputs)\n",
        "output = Dense(units=11, activation='relu')(densenet_121_model)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-49d38cc5c930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0min1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdensenet_121_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseNet121\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdensenet_121_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m             if all([s is not None\n\u001b[1;32m    467\u001b[0m                     for s in to_list(input_shape)]):\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    597\u001b[0m             raise ValueError('Invalid input_shape argument ' +\n\u001b[1;32m    598\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': model has '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                              str(len(self._input_layers)) + ' tensor inputs.')\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid input_shape argument [(None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3), (None, 32, 32, 3)]: model has 1 tensor inputs."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfYuZaIyrnGQ",
        "colab_type": "code",
        "outputId": "50552790-7d0e-481b-cf68-e7ce32cd4a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "import keras\n",
        "from keras import Input, Model\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.layers import Dense, Flatten, Concatenate\n",
        "from keras.activations import relu\n",
        "\n",
        "# Multiple inputs\n",
        "in1 = Input(shape=(128,128,3))\n",
        "in2 = Input(shape=(128,128,3))\n",
        "in3 = Input(shape=(128,128,3))\n",
        "\n",
        "# CNN output\n",
        "cnn = DenseNet121(include_top=False)\n",
        "#cnn.summary()\n",
        "\n",
        "out1 = cnn(in1)\n",
        "out2 = cnn(in2)\n",
        "out3 = cnn(in3)\n",
        "\n",
        "# Flattening the output for the dense layer\n",
        "fout1 = Flatten()(out1)\n",
        "fout2 = Flatten()(out2)\n",
        "fout3 = Flatten()(out3)\n",
        "\n",
        "# Getting the dense output\n",
        "dense = Dense(1, activation='softmax')\n",
        "\n",
        "dout1 = dense(fout1)\n",
        "dout2 = dense(fout2)\n",
        "dout3 = dense(fout3)\n",
        "\n",
        "# Concatenating the final output\n",
        "out = Concatenate(axis=-1)([dout1, dout2, dout3])\n",
        "\n",
        "# Creating the model\n",
        "model = Model(inputs=[in1,in2,in3], outputs=out)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_306 (InputLayer)          (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_307 (InputLayer)          (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_308 (InputLayer)          (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "densenet121 (Model)             multiple             7037504     input_306[0][0]                  \n",
            "                                                                 input_307[0][0]                  \n",
            "                                                                 input_308[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 16384)        0           densenet121[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 16384)        0           densenet121[2][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 16384)        0           densenet121[3][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 1)            16385       flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "                                                                 flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3)            0           dense_12[0][0]                   \n",
            "                                                                 dense_12[1][0]                   \n",
            "                                                                 dense_12[2][0]                   \n",
            "==================================================================================================\n",
            "Total params: 7,053,889\n",
            "Trainable params: 6,970,241\n",
            "Non-trainable params: 83,648\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1vXRhpz0mou",
        "colab_type": "text"
      },
      "source": [
        "# Padding = Same and Padding = Valid \n",
        "https://stackoverflow.com/questions/60323897/tensorflow-keras-conv2d-layers-with-padding-same-behave-strangely"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRHrSh0u0m5p",
        "colab_type": "code",
        "outputId": "4ecd0ad4-7a7e-469b-dc6a-c34a6a9a199a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# (1) Importing dependency\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "# (3) Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=24, input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2) ,padding='Same'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 3, 3, 24)          120       \n",
            "=================================================================\n",
            "Total params: 120\n",
            "Trainable params: 120\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rooePLzj1y5h",
        "colab_type": "code",
        "outputId": "a73f7aa2-3b5b-46e3-a405-087f68ea8dbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Importing dependency\n",
        "%tensorflow_version 1.x\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional Layer\n",
        "model.add(Conv2D(filters=24, input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2) ,padding='Valid'))\n",
        "\n",
        "# Model Summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 2, 2, 24)          120       \n",
            "=================================================================\n",
            "Total params: 120\n",
            "Trainable params: 120\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llW_6Wnp3izz",
        "colab_type": "code",
        "outputId": "c66f1c28-231e-4c32-85f0-eff16eb44b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Importing dependency\n",
        "%tensorflow_version 1.x\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional Layer\n",
        "model.add(Conv2D(filters=24, input_shape=(6,6,1), kernel_size=(2,2), strides =(2,2) ,padding='Valid'))\n",
        "\n",
        "# Model Summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 3, 3, 24)          120       \n",
            "=================================================================\n",
            "Total params: 120\n",
            "Trainable params: 120\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk6am3SF58J6",
        "colab_type": "text"
      },
      "source": [
        "# model.fit_generator Plot\n",
        "https://stackoverflow.com/questions/60306753/drawing-the-accuracy-of-multiple-validation-of-diffferent-cnn-classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_MWlH9u58Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
        "\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures\n",
        "\n",
        "num_cats_tr = len(os.listdir(train_cats_dir))\n",
        "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
        "\n",
        "num_cats_val = len(os.listdir(validation_cats_dir))\n",
        "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
        "\n",
        "total_train = num_cats_tr + num_dogs_tr\n",
        "total_val = num_cats_val + num_dogs_val\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 15\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255,brightness_range=[0.5,1.5]) # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255,brightness_range=[0.5,1.5]) # Generator for our validation data\n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "lr=0.01\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "    adam = Adam(lr)\n",
        "\n",
        "    print(\"Model using learning rate of\",lr)\n",
        "\n",
        "    lr = lr + 0.01\n",
        "\n",
        "    model.compile(optimizer=adam, \n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit_generator(\n",
        "              train_data_gen,\n",
        "              steps_per_epoch=total_train // batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=val_data_gen,\n",
        "              validation_steps=total_val // batch_size)\n",
        "    \n",
        "    plt.plot(100 * history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['LR=0.01', 'LR=0.02', 'LR=0.03', 'LR=0.04', 'LR=0.05'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rJtqLmjYX1w",
        "colab_type": "code",
        "outputId": "da76c5d1-05fd-4ec8-83f6-5d12348adada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
        "\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures\n",
        "\n",
        "num_cats_tr = len(os.listdir(train_cats_dir))\n",
        "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
        "\n",
        "num_cats_val = len(os.listdir(validation_cats_dir))\n",
        "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
        "\n",
        "total_train = num_cats_tr + num_dogs_tr\n",
        "total_val = num_cats_val + num_dogs_val\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 15\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255,brightness_range=[0.5,1.5]) # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255,brightness_range=[0.5,1.5]) # Generator for our validation data\n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "\n",
        "for i in range(7):\n",
        "\n",
        "    print(\"Model using\",optimizer[i],\"optimizer\")\n",
        "\n",
        "    model.compile(optimizer=optimizer[i], \n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit_generator(\n",
        "              train_data_gen,\n",
        "              steps_per_epoch=total_train // batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=val_data_gen,\n",
        "              validation_steps=total_val // batch_size)\n",
        "    \n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Validation Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-559fbc13edeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mvalidation_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mnum_cats_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cats_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mnum_dogs_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dogs_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_cats_dir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ip33pHpNOVU",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing the Conv3D and Conv2D Kernel\n",
        "https://stackoverflow.com/questions/60456336/weight-visualization-of-3d-convolutional-kernel\n",
        "\n",
        "Interesting Read that helps in visualizing the image after every layer - https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/\n",
        "\n",
        "Params of a layer -\n",
        "“(n* m * l+1)*k”\n",
        "- The filter size is “n*m”.\n",
        "- “l” feature maps as the input \n",
        "- “k” feature maps as output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVB8mbWfNCIF",
        "colab_type": "text"
      },
      "source": [
        "## Conv3D Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F96V691CNOeP",
        "colab_type": "code",
        "outputId": "5c3e41e7-a9b8-4c40-a1d8-f3eb8d8390d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# (1) Importing dependency\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, Conv3D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1000)\n",
        "    \n",
        "# (2) Get Data\n",
        "import tflearn.datasets.oxflower17 as oxflower17\n",
        "x, y = oxflower17.load_data(one_hot=True)\n",
        "x = np.expand_dims(x,-1)\n",
        "    \n",
        "# (3) Create a sequential model\n",
        "model = Sequential()\n",
        "    \n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv3D(filters=2, input_shape=(224,224,3,1), kernel_size=(3,3,3), strides=(4,4,4), padding='Same'))\n",
        "model.add(Activation('relu'))\n",
        "    \n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv3D(filters=4, kernel_size=(4,4,4), strides=(1,1,1), padding='Same'))\n",
        "model.add(Activation('relu'))\n",
        "    \n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv3D(filters=2, kernel_size=(4,4,4), strides=(1,1,1), padding='Same'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(17))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# (4) Compile \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x, y, batch_size=64, epochs= 4, verbose=1, validation_split=0.2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_4 (Conv3D)            (None, 56, 56, 1, 2)      56        \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 56, 56, 1, 2)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_5 (Conv3D)            (None, 56, 56, 1, 4)      516       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 56, 56, 1, 4)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_6 (Conv3D)            (None, 56, 56, 1, 2)      514       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 56, 56, 1, 2)      0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               627300    \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 17)                1717      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 17)                0         \n",
            "=================================================================\n",
            "Total params: 630,103\n",
            "Trainable params: 630,103\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 1088 samples, validate on 272 samples\n",
            "Epoch 1/4\n",
            "1088/1088 [==============================] - 1s 1ms/step - loss: 2.8280 - acc: 0.0524 - val_loss: 2.8091 - val_acc: 0.0699\n",
            "Epoch 2/4\n",
            "1088/1088 [==============================] - 1s 698us/step - loss: 2.7028 - acc: 0.1847 - val_loss: 2.5356 - val_acc: 0.2279\n",
            "Epoch 3/4\n",
            "1088/1088 [==============================] - 1s 713us/step - loss: 2.2128 - acc: 0.2978 - val_loss: 2.1192 - val_acc: 0.2574\n",
            "Epoch 4/4\n",
            "1088/1088 [==============================] - 1s 713us/step - loss: 1.7553 - acc: 0.4237 - val_loss: 1.9295 - val_acc: 0.3640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd3d006f668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-P_eKgE1yqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "x = model.layers[4].kernel\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    print(sess.run(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "19bc9c7b-f444-42da-9dcf-0eb5cb000dbd",
        "id": "g6rmQdDrNeYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# summarize filter shapes\n",
        "for layer in model.layers:\n",
        "\t# check for convolutional layer\n",
        "\tif 'conv' in layer.name:\n",
        "\t  # get filter weights\n",
        "\t  filters, biases = layer.get_weights()\n",
        "\t  print(layer.name, filters.shape)\n",
        "\t \n",
        "#print(biases)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv3d_4 (3, 3, 3, 1, 2)\n",
            "conv3d_5 (4, 4, 4, 2, 4)\n",
            "conv3d_6 (4, 4, 4, 4, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4vQ3EuZqoQi",
        "colab_type": "text"
      },
      "source": [
        "### To print Color Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2c5d4258-e392-4964-d7ad-19ca8525ba37",
        "id": "PP5fNoi1qnHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "# # summarize filter shapes\n",
        "# for layer in model.layers:\n",
        "# \t# check for convolutional layer\n",
        "# \tif 'conv' in layer.name:\n",
        "# \t  # get filter weights\n",
        "# \t  filters, biases = layer.get_weights()\n",
        "   \n",
        "# retrieve weights from the second hidden layer\n",
        "filters, biases = model.layers[4].get_weights()\n",
        "\n",
        "# normalize filter values to 0-1 so we can visualize them\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)\n",
        "# plot first few filters\n",
        "# n_filters = outgoing channels\n",
        "outgoing_channels = 2\n",
        "n_filters, ix = outgoing_channels, 1\n",
        "for i in range(n_filters):\n",
        "\t# get the filter\n",
        "\tf = filters[:, :, :, :, i]\n",
        "\t# plot each channel separately\n",
        "\t# Range of incoming channels\n",
        "\tincoming_channels = 4\n",
        "\tfor j in range(incoming_channels):\n",
        "\t\t\t# specify subplot and turn of axis\n",
        "\t\t\tax = pyplot.subplot(3, incoming_channels, ix)\n",
        "\t\t\tax.set_xticks([])\n",
        "\t\t\tax.set_yticks([])\n",
        "\t\t\t# plot filter channel in grayscale\n",
        "\t\t\tpyplot.imshow(f[:, :, :,j], cmap='gray')\n",
        "\t\t\tix += 1\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVcAAACeCAYAAACGoUnQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nngAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBM6yqw3qh3Q",
        "colab_type": "text"
      },
      "source": [
        "### To print Gray scale image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnYdGbqnZ3pg",
        "colab_type": "code",
        "outputId": "dd2d112f-229d-4299-ee37-22e8f2b63000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "# # summarize filter shapes\n",
        "# for layer in model.layers:\n",
        "# \t# check for convolutional layer\n",
        "# \tif 'conv' in layer.name:\n",
        "# \t  # get filter weights\n",
        "# \t  filters, biases = layer.get_weights()\n",
        "   \n",
        "# retrieve weights from the second hidden layer\n",
        "filters, biases = model.layers[4].get_weights()\n",
        "\n",
        "# normalize filter values to 0-1 so we can visualize them\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)\n",
        "# plot first few filters\n",
        "# n_filters = outgoing channels\n",
        "outgoing_channels = 2\n",
        "n_filters, ix = outgoing_channels, 1\n",
        "for i in range(n_filters):\n",
        "\t# get the filter\n",
        "\tf = filters[:, :, :, :, i]\n",
        "\t# plot each channel separately\n",
        "\t# Range of incoming channels\n",
        "\tincoming_channels = 4\n",
        "\tfor j in range(incoming_channels):\n",
        "\t\t# Range of Depth of the kernel .i.e. 3\n",
        "\t\tDepth = 4\n",
        "\t\tfor k in range(Depth):\n",
        "\t\t\t# specify subplot and turn of axis\n",
        "\t\t\tax = pyplot.subplot((outgoing_channels*Depth), incoming_channels, ix)\n",
        "\t\t\tax.set_xticks([])\n",
        "\t\t\tax.set_yticks([])\n",
        "\t\t\t# plot filter channel in grayscale\n",
        "\t\t\tpyplot.imshow(f[:, :, k,j], cmap='gray')\n",
        "\t\t\tix += 1\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS8AAADrCAYAAAA40BDOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 32 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4s_VHK_PNWCO"
      },
      "source": [
        "## Conv2D Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a13d4066-6386-4c6c-86c0-0d1d27415c05",
        "id": "1G6Jyc2_NP93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# (1) Importing dependency\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, Conv3D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1000)\n",
        "    \n",
        "# (2) Get Data\n",
        "import tflearn.datasets.oxflower17 as oxflower17\n",
        "x, y = oxflower17.load_data(one_hot=True)\n",
        "    \n",
        "# (3) Create a sequential model\n",
        "model = Sequential()\n",
        "    \n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=2, input_shape=(224,224,3), kernel_size=(3,3), strides=(4,4), padding='Same'))\n",
        "model.add(Activation('relu'))\n",
        "    \n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=4, kernel_size=(3,3), strides=(1,1), padding='Same'))\n",
        "model.add(Activation('relu'))\n",
        "    \n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=2, kernel_size=(3,3), strides=(1,1), padding='Same'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(17))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# (4) Compile \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "epoch_gradient = []\n",
        "\n",
        "def get_gradient_func(model):\n",
        "    grads = K.gradients(model.total_loss, model.trainable_weights)\n",
        "    inputs = model.model._feed_inputs + model.model._feed_targets + model.model._feed_sample_weights\n",
        "    func = K.function(inputs, grads)\n",
        "    return func\n",
        "\n",
        "# Define the Required Callback Function\n",
        "class GradientCalcCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "      get_gradient = get_gradient_func(model)\n",
        "      grads = get_gradient([x, y, np.ones(len(y))])\n",
        "      epoch_gradient.append(grads)\n",
        "    \n",
        "epoch = 4\n",
        "\n",
        "model.fit(x, y, batch_size=64, epochs= epoch, verbose=1, validation_split=0.2, shuffle=True, callbacks=[GradientCalcCallback()])\n",
        "    \n",
        "# (7) Convert to a 2 dimensiaonal array of (epoch, gradients) type\n",
        "gradient = np.asarray(epoch_gradient)\n",
        "print(\"Total number of epochs run:\", epoch)\n",
        "print(\"Gradient Array has the shape:\",gradient.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Downloading Oxford 17 category Flower Dataset, Please wait...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100.0% 60276736 / 60270631\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('Succesfully downloaded', '17flowers.tgz', 60270631, 'bytes.')\n",
            "File Extracted\n",
            "Starting to parse images...\n",
            "Parsing Done!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 56, 56, 2)         56        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 56, 56, 2)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 56, 56, 4)         76        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 56, 56, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 56, 56, 2)         74        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 56, 56, 2)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               627300    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 17)                1717      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 17)                0         \n",
            "=================================================================\n",
            "Total params: 629,223\n",
            "Trainable params: 629,223\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 1088 samples, validate on 272 samples\n",
            "Epoch 1/4\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1088/1088 [==============================] - 14s 13ms/step - loss: 2.8319 - acc: 0.0533 - val_loss: 2.8246 - val_acc: 0.0662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/4\n",
            "1088/1088 [==============================] - 1s 567us/step - loss: 2.7434 - acc: 0.2426 - val_loss: 2.7336 - val_acc: 0.1250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/4\n",
            "1088/1088 [==============================] - 1s 579us/step - loss: 2.2730 - acc: 0.4384 - val_loss: 2.4033 - val_acc: 0.2794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/4\n",
            "1088/1088 [==============================] - 1s 555us/step - loss: 1.3170 - acc: 0.6884 - val_loss: 2.2917 - val_acc: 0.2794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
            "  warnings.warn('`Sequential.model` is deprecated. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total number of epochs run: 4\n",
            "Gradient Array has the shape: (4, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwSYQQEW9vDj",
        "colab_type": "code",
        "outputId": "8cde4e2a-88f5-4416-9858-8523bfcfdb09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "import numpy as np\n",
        "x = model.layers[4].kernel\n",
        "gr = tf.get_default_graph()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    conv1_kernel_val = gr.get_tensor_by_name('conv1/kernel:0').eval()\n",
        "    print(sess.run(x))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[-0.07628679  0.31725606]\n",
            "   [-0.29280648  0.10707554]\n",
            "   [-0.1409379  -0.14117424]\n",
            "   [-0.04934797 -0.17769067]]\n",
            "\n",
            "  [[-0.22811215  0.2088342 ]\n",
            "   [ 0.12845007 -0.10234594]\n",
            "   [ 0.29518536  0.22939327]\n",
            "   [-0.102254   -0.16633217]]\n",
            "\n",
            "  [[ 0.23566589  0.06252447]\n",
            "   [ 0.12485743 -0.2845057 ]\n",
            "   [ 0.17910227 -0.09911792]\n",
            "   [ 0.09142479 -0.17152238]]]\n",
            "\n",
            "\n",
            " [[[-0.23811015 -0.16541886]\n",
            "   [-0.33333215 -0.13665256]\n",
            "   [ 0.09643212  0.07662854]\n",
            "   [ 0.05305314 -0.23696613]]\n",
            "\n",
            "  [[ 0.0339109   0.16784325]\n",
            "   [-0.04169551 -0.0482665 ]\n",
            "   [ 0.08657351  0.12973014]\n",
            "   [ 0.05880824  0.05947757]]\n",
            "\n",
            "  [[ 0.11017862 -0.14729127]\n",
            "   [ 0.2464253  -0.19549426]\n",
            "   [-0.2609269   0.25060275]\n",
            "   [-0.26333413 -0.13276713]]]\n",
            "\n",
            "\n",
            " [[[-0.06246376  0.0363799 ]\n",
            "   [-0.02949128  0.32878068]\n",
            "   [ 0.15073076 -0.30452785]\n",
            "   [-0.24252614 -0.09735529]]\n",
            "\n",
            "  [[ 0.23945466  0.01562142]\n",
            "   [ 0.22383246  0.09923801]\n",
            "   [-0.2536789  -0.09304142]\n",
            "   [-0.22144732  0.3057051 ]]\n",
            "\n",
            "  [[ 0.05687061 -0.2811746 ]\n",
            "   [-0.2436115  -0.1616823 ]\n",
            "   [ 0.06709924 -0.24549572]\n",
            "   [ 0.04901949 -0.06542149]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4a22d8ba-a297-43c2-b58a-36280e0b695c",
        "id": "lQneZbNRNP98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# summarize filter shapes\n",
        "for layer in model.layers:\n",
        "\t# check for convolutional layer\n",
        "\tif 'conv' in layer.name:\n",
        "\t  # get filter weights\n",
        "\t  filters, biases = layer.get_weights()\n",
        "\t  print(layer.name, filters.shape)\n",
        "\t \n",
        "#print(filters[:,:,:,0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2d_4 (3, 3, 3, 2)\n",
            "conv2d_5 (3, 3, 2, 4)\n",
            "conv2d_6 (3, 3, 4, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d82d2727-0fe7-4a83-cf25-de643d609776",
        "id": "xnNFITm2NP-E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "# # summarize filter shapes\n",
        "# for layer in model.layers:\n",
        "# \t# check for convolutional layer\n",
        "# \tif 'conv' in layer.name:\n",
        "# \t  # get filter weights\n",
        "# \t  filters, biases = layer.get_weights()\n",
        "   \n",
        "# retrieve weights from the second hidden layer\n",
        "filters, biases = model.layers[4].get_weights()\n",
        "\n",
        "# normalize filter values to 0-1 so we can visualize them\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)\n",
        "# plot first few filters\n",
        "# n_filters = outgoing filters\n",
        "n_filters, ix = 2, 1 \n",
        "for i in range(n_filters):\n",
        "\t# get the filter\n",
        "\tf = filters[:, :, :, i]\n",
        "\t#print(f)\n",
        "\t# plot each channel separately\n",
        "\t# Range of incoming filters\n",
        "\tfor j in range(4):\n",
        "\t\t# specify subplot and turn of axis\n",
        "\t\tax = pyplot.subplot(3, 3, ix)\n",
        "\t\tax.set_xticks([])\n",
        "\t\tax.set_yticks([])\n",
        "\t\t# plot filter channel in grayscale\n",
        "\t\tpyplot.imshow(f[:, :, j], cmap='gray')\n",
        "\t  #print(f[:, :, j])\n",
        "\t\tix += 1\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAADrCAYAAADwvPoYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjw2f-GyThTk",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow Reduce_max\n",
        "https://stackoverflow.com/questions/60277848/tensorflow-reduce-max-for-different-dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-OCxARrThgJ",
        "colab_type": "code",
        "outputId": "d9069138-1565-411e-d052-fab527b16df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a Ragged Tensor of variable length\n",
        "rt = tf.ragged.constant([[9, 8, 7], [], [6, 5], [4]])\n",
        "print(\"Ragged Tensor:\",\"\\n\",rt,\"\\n\")\n",
        "\n",
        "# Convert to Tensor to have same length\n",
        "rt = rt.to_tensor()\n",
        "print(\"Tensor of same length:\",\"\\n\",rt,\"\\n\")\n",
        "\n",
        "# Apply reduce_max to get the max value along axis=1\n",
        "rt = tf.reduce_max(rt, axis=1)\n",
        "print(\"Reduce Max Tensor:\",\"\\n\",rt,\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ragged Tensor: \n",
            " <tf.RaggedTensor [[9, 8, 7], [], [6, 5], [4]]> \n",
            "\n",
            "Tensor of same length: \n",
            " tf.Tensor(\n",
            "[[9 8 7]\n",
            " [0 0 0]\n",
            " [6 5 0]\n",
            " [4 0 0]], shape=(4, 3), dtype=int32) \n",
            "\n",
            "Reduce Max Tensor: \n",
            " tf.Tensor([9 0 6 4], shape=(4,), dtype=int32) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzPFRXDg4Di0",
        "colab_type": "text"
      },
      "source": [
        "# Save and Load Model\n",
        "https://stackoverflow.com/questions/60198878/proper-way-to-save-model-in-keras\n",
        "\n",
        "Good Article for Load and Save in Keras - https://machinelearningmastery.com/save-load-keras-deep-learning-models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kEKp1C76nw6",
        "colab_type": "text"
      },
      "source": [
        "## Build and Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeSykV1cqtSm",
        "colab_type": "code",
        "outputId": "9278f3e5-fef0-424a-c74a-2e0d47d343f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "# MLP for Pima Indians Dataset saved to single file\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = np.loadtxt(\"/content/pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# save model and architecture to single file\n",
        "model.save(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 12)                108       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 104       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 221\n",
            "Trainable params: 221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "accuracy: 75.78%\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWYVNZs-6ujU",
        "colab_type": "text"
      },
      "source": [
        "## Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw7iWQwy5KP8",
        "colab_type": "code",
        "outputId": "c925bc56-29c0-4bab-8623-52856e91661d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# load and evaluate a saved model\n",
        "from numpy import loadtxt\n",
        "from keras.models import load_model\n",
        " \n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# summarize model.\n",
        "model.summary()\n",
        "\n",
        "# load dataset\n",
        "dataset = loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# evaluate the model\n",
        "score = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 12)                108       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 104       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 221\n",
            "Trainable params: 221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "accuracy: 75.78%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fARWvSOS0xo",
        "colab_type": "text"
      },
      "source": [
        "# Feed Dict Example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYzDEyNw-qYu",
        "colab_type": "code",
        "outputId": "8adf273d-edbd-4d44-8412-f382940ad0de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = x * 42\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  train_accuracy = y.eval(session=sess,feed_dict={x: (2, 4)})\n",
        "  print(train_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 84. 168.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUvdRR4EyO6-",
        "colab_type": "text"
      },
      "source": [
        "# Dealing with Session Error Explained\n",
        "https://stackoverflow.com/questions/61006702/cannot-use-the-given-session-to-evaluate-tensor-the-tensors-graph-is-different"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj1v2ptOIvyc",
        "colab_type": "text"
      },
      "source": [
        "## Simple Error and Fix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eybmrlAanGFK",
        "colab_type": "code",
        "outputId": "765073db-13b0-4c71-b4fe-a04db6b5d3be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  x = tf.constant(1.0)  # x is created in graph g\n",
        "\n",
        "s = tf.Session(graph=g)\n",
        "with s.as_default() as sess:\n",
        "  print(x.eval()) # x was created in graph g and it is evaluated in session s\n",
        "                  # which is tied to graph g, so everything is ok.\n",
        "  y = tf.constant(2.0) # y is created in TensorFlow's default graph!!!\n",
        "  print(y.eval()) # y was created in TF's default graph, but it is evaluated in\n",
        "                  # session s which is tied to graph g => ERROR"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-56f6710c85ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                   \u001b[0;31m# which is tied to graph g, so everything is ok.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# y is created in TensorFlow's default graph!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# y was created in TF's default graph, but it is evaluated in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                   \u001b[0;31m# session s which is tied to graph g => ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \"\"\"\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexperimental_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5396\u001b[0m                        \"`eval(session=sess)`\")\n\u001b[1;32m   5397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5398\u001b[0;31m       raise ValueError(\"Cannot use the default session to evaluate tensor: \"\n\u001b[0m\u001b[1;32m   5399\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5400\u001b[0m                        \u001b[0;34m\"graph. Pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhZjS6zEqHYg",
        "colab_type": "code",
        "outputId": "cdbf2509-90ca-4d6b-f90e-ce9741d8a828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  x = tf.constant(1.0)  # x is created in graph g\n",
        "  y = tf.constant(2.0) # y is created in graph g\n",
        "\n",
        "s = tf.Session(graph=g)\n",
        "with s.as_default() as sess:\n",
        "  print(x.eval()) # x was created in graph g and it is evaluated in session s\n",
        "                         # which is tied to graph g, so everything is ok.\n",
        "  print(y.eval()) # y was created in graph g and it is evaluated in session s\n",
        "                         # which is tied to graph g, so everything is ok."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssuiDS6xJxkn",
        "colab_type": "text"
      },
      "source": [
        "## Error and Fix Explained in Detail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvI2gcXuUZ_A",
        "colab_type": "text"
      },
      "source": [
        "### Error with default session and using variable created in another graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c68196dd-39c9-4e4f-b074-9630c2d3d207",
        "id": "_MjHucq4TEJ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  x = tf.constant(1.0)  # x is created in graph g\n",
        "\n",
        "with tf.Session().as_default() as sess:\n",
        "  y = tf.constant(2.0) # y is created in TensorFlow's default graph!!!\n",
        "  print(y.eval(session=sess)) # y was created in TF's default graph, and is evaluated in\n",
        "                  # default session, so everything is ok.  \n",
        "  print(x.eval(session=sess)) # x was created in graph g and it is evaluated in session s\n",
        "                  # which is tied to graph g, but it is evaluated in\n",
        "                  # session s which is tied to graph g => ERROR"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f35cb204cf59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# y was created in TF's default graph, and is evaluated in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                   \u001b[0;31m# default session, so everything is ok.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# x was created in graph g and it is evaluated in session s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                   \u001b[0;31m# which is tied to graph g, but it is evaluated in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                   \u001b[0;31m# session s which is tied to graph g => ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \"\"\"\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexperimental_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5402\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5404\u001b[0;31m       raise ValueError(\"Cannot use the given session to evaluate tensor: \"\n\u001b[0m\u001b[1;32m   5405\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5406\u001b[0m                        \"graph.\")\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyAx1Zp1Us4F",
        "colab_type": "text"
      },
      "source": [
        "### Error with graph session as default and using variable created in default graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og4q09R-GHlL",
        "colab_type": "code",
        "outputId": "08030e4e-bd58-4126-bad6-51e9a8e86096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  x = tf.constant(1.0)  # x is created in graph g\n",
        "\n",
        "with tf.Session(graph=g).as_default() as sess:\n",
        "  print(x.eval(session=sess)) # x was created in graph g and it is evaluated in session s\n",
        "                         # which is tied to graph g, so everything is ok.\n",
        "  y = tf.constant(2.0) # y is created in TensorFlow's default graph!!!\n",
        "  print(y.eval()) # y was created in TF's default graph, but it is evaluated in\n",
        "                  # session s which is tied to graph g => ERROR"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6b8b687c5178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                          \u001b[0;31m# which is tied to graph g, so everything is ok.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# y is created in TensorFlow's default graph!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# y was created in TF's default graph, but it is evaluated in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                   \u001b[0;31m# session s which is tied to graph g => ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \"\"\"\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexperimental_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5396\u001b[0m                        \"`eval(session=sess)`\")\n\u001b[1;32m   5397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5398\u001b[0;31m       raise ValueError(\"Cannot use the default session to evaluate tensor: \"\n\u001b[0m\u001b[1;32m   5399\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5400\u001b[0m                        \u001b[0;34m\"graph. Pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4ecy2vH3X6gU"
      },
      "source": [
        "### Error with graph session as default and using variable created in default graph and also session=sess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6281a25e-0b0e-48a1-f713-0dc083f4d347",
        "id": "EoznhA2nX6gW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  x = tf.constant(1.0)  # x is created in graph g\n",
        "\n",
        "with tf.Session(graph=g).as_default() as sess:\n",
        "  print(x.eval(session=sess)) # x was created in graph g and it is evaluated in session s\n",
        "                         # which is tied to graph g, so everything is ok.\n",
        "  y = tf.constant(2.0) # y is created in TensorFlow's default graph!!!\n",
        "  print(y.eval(session=sess)) # y was created in TF's default graph, but it is evaluated in\n",
        "                  # session s which is tied to graph g => ERROR"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-83809aa4e485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                          \u001b[0;31m# which is tied to graph g, so everything is ok.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# y is created in TensorFlow's default graph!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# y was created in TF's default graph, but it is evaluated in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                   \u001b[0;31m# session s which is tied to graph g => ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \"\"\"\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexperimental_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5402\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5404\u001b[0;31m       raise ValueError(\"Cannot use the given session to evaluate tensor: \"\n\u001b[0m\u001b[1;32m   5405\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5406\u001b[0m                        \"graph.\")\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0zJAoNLU7pu",
        "colab_type": "text"
      },
      "source": [
        "### Fix with default session and variable not assigned to any graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "42c47425-d833-4f5b-f9f3-196dbf8bef18",
        "id": "5Nx158TXTCT1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.constant(1.0)  # x is in not assigned to any graph\n",
        "\n",
        "with tf.Session().as_default() as sess:\n",
        "  y = tf.constant(2.0) # y is created in TensorFlow's default graph!!!\n",
        "  print(y.eval(session=sess)) # y was created in TF's default graph, and is evaluated in\n",
        "                  # default session, so everything is ok.  \n",
        "  print(x.eval(session=sess)) # x not assigned to any graph, and is evaluated in\n",
        "                  # default session, so everything is ok.  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u1qthgoVMUL",
        "colab_type": "text"
      },
      "source": [
        "### The best fix is to cleanly separate the construction phase and the execution phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UftJHIz3VM4d",
        "colab_type": "code",
        "outputId": "40824b97-fc1e-4135-ada3-a2f657975046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  x = tf.constant(1.0)  # x is created in graph g\n",
        "  y = tf.constant(2.0) # y is created in graph g\n",
        "\n",
        "with tf.Session(graph=g).as_default() as sess:\n",
        "  print(x.eval()) # x was created in graph g and it is evaluated in session s\n",
        "                         # which is tied to graph g, so everything is ok.\n",
        "  print(y.eval()) # y was created in graph g and it is evaluated in session s\n",
        "                         # which is tied to graph g, so everything is ok."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x-CnHNIH0r1",
        "colab_type": "text"
      },
      "source": [
        "# softmax and log_softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odVe5AwJH023",
        "colab_type": "code",
        "outputId": "5624739e-54a9-4bf0-fe45-ffe44020d124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "x = tf.nn.softmax([0.12345,0.3256,0.2356,-0.3256,0.13562])\n",
        "y = -tf.nn.log_softmax([0.12345,0.3256,0.2356,-0.3256,0.13562])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "   print(x.eval())\n",
        "   print(y.eval())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.20027274 0.24514017 0.22404124 0.12782091 0.20272495]\n",
            "[1.6080751 1.4059252 1.4959252 2.057125  1.5959052]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDqxUBulbZua",
        "colab_type": "text"
      },
      "source": [
        "# Tensor to array\n",
        "https://stackoverflow.com/questions/59875172/typeerror-when-trying-to-use-earlystopping-with-f1-metric-as-stopping-criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syOHTseMZN5F",
        "colab_type": "code",
        "outputId": "9adb1ba5-0d4e-443a-e71b-468f7661ec19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "print(tf.__version__)\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "x = tf.constant([1,2,3,4,5,6])\n",
        "print(\"Type of x:\",x)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  y = np.array(x.eval())\n",
        "  print(\"Type of y:\",y.shape,y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n",
            "Type of x: Tensor(\"Const_24:0\", shape=(6,), dtype=int32)\n",
            "Type of y: (6,) [1 2 3 4 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yg9AIIvdD7b",
        "colab_type": "text"
      },
      "source": [
        "# Fit_generator simple example.\n",
        "\n",
        "https://stackoverflow.com/questions/59417210/keras-losing-axis-with-brightness-range-during-image-augmentation\n",
        "\n",
        "\n",
        "Also example on hanling list. You can find it during Visualization where I have mulitplied list value by 100.\n",
        "\n",
        "https://stackoverflow.com/questions/61080410/cnn-accuracy-y-axis-range"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4b1ac404-0b84-4e1e-a583-b7c2ed854848",
        "id": "iJ2O5F28dF3i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
        "\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures\n",
        "\n",
        "num_cats_tr = len(os.listdir(train_cats_dir))\n",
        "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
        "\n",
        "num_cats_val = len(os.listdir(validation_cats_dir))\n",
        "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
        "\n",
        "total_train = num_cats_tr + num_dogs_tr\n",
        "total_val = num_cats_val + num_dogs_val\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 15\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255,brightness_range=[0.5,1.5]) # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255,brightness_range=[0.5,1.5]) # Generator for our validation data\n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", \n",
        "          loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "          metrics=['accuracy'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "          train_data_gen,\n",
        "          steps_per_epoch=total_train // batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=val_data_gen,\n",
        "          validation_steps=total_val // batch_size)\n",
        "\n",
        "val_accuracy = [i * 100 for i in history.history['val_accuracy']]\n",
        "plt.plot(val_accuracy)\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Val Accuracy'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "68608000/68606236 [==============================] - 1s 0us/step\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "WARNING:tensorflow:From <ipython-input-32-0eb8b71d03a3>:74: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/15\n",
            "15/15 [==============================] - 10s 689ms/step - loss: 0.9475 - accuracy: 0.5068 - val_loss: 0.6932 - val_accuracy: 0.5022\n",
            "Epoch 2/15\n",
            "15/15 [==============================] - 11s 700ms/step - loss: 0.6876 - accuracy: 0.5021 - val_loss: 0.6813 - val_accuracy: 0.4955\n",
            "Epoch 3/15\n",
            "15/15 [==============================] - 11s 733ms/step - loss: 0.6599 - accuracy: 0.5529 - val_loss: 0.6421 - val_accuracy: 0.6105\n",
            "Epoch 4/15\n",
            "15/15 [==============================] - 11s 748ms/step - loss: 0.6041 - accuracy: 0.6480 - val_loss: 0.6185 - val_accuracy: 0.6920\n",
            "Epoch 5/15\n",
            "15/15 [==============================] - 10s 697ms/step - loss: 0.5682 - accuracy: 0.6843 - val_loss: 0.6029 - val_accuracy: 0.6540\n",
            "Epoch 6/15\n",
            "15/15 [==============================] - 11s 707ms/step - loss: 0.5207 - accuracy: 0.7372 - val_loss: 0.5908 - val_accuracy: 0.6719\n",
            "Epoch 7/15\n",
            "15/15 [==============================] - 11s 708ms/step - loss: 0.4971 - accuracy: 0.7404 - val_loss: 0.6015 - val_accuracy: 0.6752\n",
            "Epoch 8/15\n",
            "15/15 [==============================] - 10s 692ms/step - loss: 0.4628 - accuracy: 0.7740 - val_loss: 0.5982 - val_accuracy: 0.7042\n",
            "Epoch 9/15\n",
            "15/15 [==============================] - 11s 701ms/step - loss: 0.4281 - accuracy: 0.7837 - val_loss: 0.6484 - val_accuracy: 0.6484\n",
            "Epoch 10/15\n",
            "15/15 [==============================] - 10s 689ms/step - loss: 0.4054 - accuracy: 0.7965 - val_loss: 0.6130 - val_accuracy: 0.6775\n",
            "Epoch 11/15\n",
            "15/15 [==============================] - 11s 703ms/step - loss: 0.3732 - accuracy: 0.8178 - val_loss: 0.6456 - val_accuracy: 0.7165\n",
            "Epoch 12/15\n",
            "15/15 [==============================] - 11s 702ms/step - loss: 0.3402 - accuracy: 0.8365 - val_loss: 0.6787 - val_accuracy: 0.6540\n",
            "Epoch 13/15\n",
            "15/15 [==============================] - 10s 695ms/step - loss: 0.3025 - accuracy: 0.8627 - val_loss: 0.7162 - val_accuracy: 0.7031\n",
            "Epoch 14/15\n",
            "15/15 [==============================] - 10s 682ms/step - loss: 0.2735 - accuracy: 0.8798 - val_loss: 0.7543 - val_accuracy: 0.6797\n",
            "Epoch 15/15\n",
            "15/15 [==============================] - 10s 697ms/step - loss: 0.2449 - accuracy: 0.8819 - val_loss: 0.8991 - val_accuracy: 0.6652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV0G_dRbOUoa",
        "colab_type": "code",
        "outputId": "4132f3be-b163-484f-89a9-059249453b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "val_accuracy = [i * 100 for i in history.history['val_accuracy']]\n",
        "plt.plot(val_accuracy)\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Val Accuracy'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwhjPAkYvkBV",
        "colab_type": "text"
      },
      "source": [
        "# Convert Functional Model to Sequential and Vice Versa\n",
        "https://stackoverflow.com/questions/61130836/convert-functional-model-to-sequential-keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWjlUrHl0Nr1",
        "colab_type": "text"
      },
      "source": [
        "## User Code has one functional model and another sequential model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbKDJJn5vka7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import layers, models, applications, Input, Model\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, UpSampling2D\n",
        "\n",
        "#load in data using imagedatagenreator\n",
        "input_img = Input(shape=(128, 128,3))\n",
        "\n",
        "x = Convolution2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Convolution2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Convolution2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# at this point the representation is (8, 4, 4) i.e. 128-dimensional\n",
        "x = Convolution2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Convolution2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Convolution2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Convolution2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "#compile and run\n",
        "\n",
        "##save weights and and model start conv network with these weights\n",
        "encoder = Model(input_img, encoded)\n",
        "encoder.save('Encoded.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-lHnunPzC4q",
        "colab_type": "code",
        "outputId": "e229e05b-a444-475a-c065-29f517502e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import layers, models, applications, Input, Model, Sequential\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, UpSampling2D, Conv2D, Dense, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#load in data using imagedatagenreator\n",
        "x = load_model('Encoded.h5')\n",
        "x.summary()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(encoded)\n",
        "model.add(Conv2D(64,(3,3), input_shape=(424,424,3), activation='relu'))#3x3 is default\n",
        "model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "#model.add(Dropout(.1))#test\n",
        "model.add(Dense(32, activation='relu'))#test\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))#input_shape=(424,424,3)\n",
        "model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(.3))#test\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))#input_shape=(424,424,3)\n",
        "model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model.add(Dropout(.3))\n",
        "model.add(Flatten(input_shape=(424,424,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_87 (Conv2D)           (None, 128, 128, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_72 (MaxPooling (None, 64, 64, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_88 (Conv2D)           (None, 64, 64, 8)         1160      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_73 (MaxPooling (None, 32, 32, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_89 (Conv2D)           (None, 32, 32, 8)         584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_74 (MaxPooling (None, 16, 16, 8)         0         \n",
            "=================================================================\n",
            "Total params: 2,192\n",
            "Trainable params: 2,192\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-abc7db2e7aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m424\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m424\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#3x3 is default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    172\u001b[0m       raise TypeError('The added layer must be '\n\u001b[1;32m    173\u001b[0m                       \u001b[0;34m'an instance of class Layer. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                       'Found: ' + str(layer))\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: Tensor(\"max_pooling2d_74/Identity:0\", shape=(None, 16, 16, 8), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n9_kieW013Bh"
      },
      "source": [
        "## Convert functional model to sequential model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dFclXAGr1u3Q",
        "outputId": "2aad0bd2-ed53-41c4-dbeb-a978e21b6d89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import layers, models, applications, Input, Model\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, UpSampling2D\n",
        "\n",
        "# Create the Sequential Model\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(16, (3, 3), input_shape=(424,424,3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "model.add(Convolution2D(8, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "model.add(Convolution2D(8, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Save the Model and Architecture\n",
        "model.save('Encoded.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_60 (Conv2D)           (None, 424, 424, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 212, 212, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 212, 212, 8)       1160      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 106, 106, 8)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 106, 106, 8)       584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 53, 53, 8)         0         \n",
            "=================================================================\n",
            "Total params: 2,192\n",
            "Trainable params: 2,192\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0b3bd82b-5ec8-4264-a6a0-ab73bf2b647c",
        "id": "PIp5DMAb1u3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import layers, models, applications, Input, Model, Sequential\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, UpSampling2D, Conv2D, Dense, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the previoulsy saved enocdermodel \n",
        "model = load_model('Encoded.h5')\n",
        "\n",
        "# Add the additonal layers \n",
        "model.add(Conv2D(64,(3,3), activation='relu'))#3x3 is default\n",
        "model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "#model.add(Dropout(.1))#test\n",
        "model.add(Dense(32, activation='relu'))#test\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))#input_shape=(424,424,3)\n",
        "model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(.3))#test\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))#input_shape=(424,424,3)\n",
        "model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model.add(Dropout(.3))\n",
        "model.add(Flatten(input_shape=(424,424,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Model summary \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_60 (Conv2D)           (None, 424, 424, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 212, 212, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 212, 212, 8)       1160      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 106, 106, 8)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 106, 106, 8)       584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 53, 53, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_63 (Conv2D)           (None, 51, 51, 64)        4672      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling (None, 17, 17, 64)        0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 17, 17, 32)        2080      \n",
            "_________________________________________________________________\n",
            "conv2d_64 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 5, 5, 64)          4160      \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_65 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 68,914\n",
            "Trainable params: 68,786\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XjDDrht_4wca"
      },
      "source": [
        "## Convert sequential model to functional model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "283hoBLI49D4",
        "outputId": "e1e91253-5ea9-469b-b632-12ffa2414a2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import layers, models, applications, Input, Model\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, UpSampling2D\n",
        "\n",
        "#load in data using imagedatagenreator\n",
        "input_img = Input(shape=(424,424,3))\n",
        "\n",
        "x = Convolution2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Convolution2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Convolution2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "##save weights and and model start conv network with these weights\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# Model Summary\n",
        "encoder.summary()\n",
        "\n",
        "encoder.save('Encoded.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 424, 424, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_66 (Conv2D)           (None, 424, 424, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 212, 212, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_67 (Conv2D)           (None, 212, 212, 8)       1160      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_52 (MaxPooling (None, 106, 106, 8)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_68 (Conv2D)           (None, 106, 106, 8)       584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_53 (MaxPooling (None, 53, 53, 8)         0         \n",
            "=================================================================\n",
            "Total params: 2,192\n",
            "Trainable params: 2,192\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e09aee33-71c0-4197-8acc-e2635825bd46",
        "id": "tlicujIp4wcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import layers, models, applications, Input, Model, Sequential\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, UpSampling2D, Conv2D, Dense, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the previoulsy saved enocdermodel \n",
        "load_model('Encoded.h5')\n",
        "\n",
        "# Add the additonal layers \n",
        "x = Convolution2D(64,(3,3), activation='relu')(encoded)#3x3 is default\n",
        "x = MaxPooling2D(pool_size=(3,3))(x)\n",
        "#model.add(Dropout(.1))#test\n",
        "x = Dense(32, activation='relu')(x)#test\n",
        "x = Conv2D(64,(3,3), activation='relu')(x)#input_shape=(424,424,3)\n",
        "x = MaxPooling2D(pool_size=(3,3))(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(.3)(x)#test\n",
        "x = Conv2D(64,(3,3), activation='relu')(x)#input_shape=(424,424,3)\n",
        "x = MaxPooling2D(pool_size=(3,3))(x)\n",
        "x = Dropout(.3)(x)\n",
        "x = Flatten(input_shape=(424,424,3))(x)\n",
        "x = BatchNormalization()(x)\n",
        "output = Dense(2, activation='softmax')(x)\n",
        "\n",
        "##save weights and and model start conv network with these weights\n",
        "model = Model(input_img, output)\n",
        "\n",
        "# Model summary \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 424, 424, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 424, 424, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling (None, 212, 212, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 212, 212, 8)       1160      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling (None, 106, 106, 8)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 106, 106, 8)       584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling (None, 53, 53, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_57 (Conv2D)           (None, 51, 51, 64)        4672      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling (None, 17, 17, 64)        0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 17, 17, 32)        2080      \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 5, 5, 64)          4160      \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 68,914\n",
            "Trainable params: 68,786\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr6jOpefsMfJ",
        "colab_type": "text"
      },
      "source": [
        "# Custom Loss Function Example\n",
        "https://stackoverflow.com/questions/59415275/custom-metric-in-multi-output-keras-model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iqSwN-KLsMyo",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define custom loss\n",
        "# Create a loss function that adds the MSE loss to the mean of all squared activations\n",
        "def custom_loss(y_pred1, y_true1, y_pred2, y_true2):\n",
        "    Loss = 0\n",
        "\n",
        "    def loss1(y_true1,y_pred1):\n",
        "        return np.square(np.subtract(y_true1,y_pred1)).mean()\n",
        "\n",
        "    def loss2(y_true2,y_pred2):\n",
        "        return np.square(np.subtract(y_true2,y_pred2)).mean()\n",
        "\n",
        "    def finalloss(y_pred1, y_true1, y_pred2, y_true2):\n",
        "        Loss = loss1(y_pred1, y_true1) + loss2(y_pred2, y_true2)\n",
        "        if(y_pred1 == y_true1 and y_pred2 == y_true2):\n",
        "           return(0)\n",
        "        elif(y_pred1 == y_true1 and y_pred2 != y_true2):\n",
        "            return(0.5 * Loss)\n",
        "        elif(y_pred1 != y_true1 and y_pred2 == y_true2):\n",
        "            return(0.5 * Loss)    \n",
        "        else:\n",
        "            return(Loss)\n",
        "    \n",
        "    return finalloss(y_pred1, y_true1, y_pred2, y_true2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d6355f1a-9e19-4e4c-9cd2-1c864c8386d3",
        "id": "WS7_W346sMyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "custom_loss(1,1,7,2)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}